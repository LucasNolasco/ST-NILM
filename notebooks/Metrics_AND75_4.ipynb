{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Metrics_AND75_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQaSQNQQvbo"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Evaluates a trained model accordingly to the metrics specified on the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCTCtekCQyv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f34c034-19d1-4a52-f64d-e640d1e0ac2b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.4.0\n",
        "!pip install keras==2.4.0\n",
        "!pip install kymatio\n",
        "!pip install tqdm\n",
        "!pip install iterative-stratification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8RYHVHGDHAdo",
        "outputId": "309709b5-7e81-4a18-a7e8-1ea1b377cc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 17 kB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68714 sha256=9bd5eb46ff6d794fee2e37fad477b836a4f6172aad64d9a7b34b55e16eb46500\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.4.0\n",
            "  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.13)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.3.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.15.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.2.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "Successfully installed keras-2.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kymatio\n",
            "  Downloading kymatio-0.2.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kymatio) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.1)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.4)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kymatio) (3.0.9)\n",
            "Installing collected packages: configparser, kymatio\n",
            "Successfully installed configparser-5.2.0 kymatio-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER7854z7Qvbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2b8b12-35fe-4724-9dd2-b1f6cc15bd69"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"drive/MyDrive/Scattering_Novo/src\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix \n",
        "from sklearn.model_selection import train_test_split\n",
        "from ModelHandler import ModelHandler\n",
        "import pickle\n",
        "import h5py\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score     \n",
        "from tqdm import tqdm\n",
        "\n",
        "configs = {\n",
        "    \"N_GRIDS\": 5, \n",
        "    \"SIGNAL_BASE_LENGTH\": 12800, \n",
        "    \"N_CLASS\": 26, \n",
        "    \"USE_NO_LOAD\": False, \n",
        "    \"AUGMENTATION_RATIO\": 5, \n",
        "    \"MARGIN_RATIO\": 0.15, \n",
        "    \"DATASET_PATH\": \"../Synthetic_Full_iHall.hdf5\",\n",
        "    \"TRAIN_SIZE\": 0.8,\n",
        "    \"FOLDER_PATH\": \"drive/MyDrive/Scattering_Novo/tmp/DIFDUAL/tests/AND75_4/\", \n",
        "    \"FOLDER_DATA_PATH\": \"drive/MyDrive/Scattering_Novo/tmp/Without_Detection_Without_HAND/ND100/\", \n",
        "    \"N_EPOCHS_TRAINING\": 250,\n",
        "    \"INITIAL_EPOCH\": 0,\n",
        "    \"TOTAL_MAX_EPOCHS\": 250,\n",
        "    \"SNRdb\": None # Noise level on db\n",
        "}\n",
        "\n",
        "folderPath = configs[\"FOLDER_PATH\"]\n",
        "folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n",
        "signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n",
        "ngrids = configs[\"N_GRIDS\"]\n",
        "trainSize = configs[\"TRAIN_SIZE\"]\n",
        "\n",
        "dict_data = pickle.load(open(folderDataPath + \"data.p\", \"rb\")) # Load data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vomuxuyDZ9Pe"
      },
      "source": [
        "## Choose best performing model\n",
        "\n",
        "At this point, the model with best performance under the validation set is chosen.\n",
        "\n",
        "In order to make this choice, the average between f1 macro is verified.\n",
        "\n",
        "$$\n",
        "F_1 = \\frac{F1_{ON} + F1_{OFF} + F1_{NO EVENT}}{3}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwe09boZZ81Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9962db7a-47c9-4773-f8d8-a65615aa8afb"
      },
      "source": [
        "def choose_model(dict_data, folderPath):\n",
        "    from tqdm import tqdm\n",
        "    from sklearn.preprocessing import MaxAbsScaler\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score   \n",
        "    from PostProcessing import PostProcessing\n",
        "\n",
        "    scattering_extract = ModelHandler.loadModel(configs[\"FOLDER_PATH\"] + 'scattering_model.h5') # Load scattering model\n",
        "\n",
        "    threshold = 0.5\n",
        "    f1_macro, f1_micro = [], []\n",
        "    for fold in tqdm(range(1, 11)):\n",
        "        foldFolderPath = folderPath + str(fold) + \"/\"\n",
        "        \n",
        "        train_index = np.load(foldFolderPath + \"train_index.npy\")\n",
        "        validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n",
        "\n",
        "        bestModel = ModelHandler.loadModel(foldFolderPath + \"model_without_detection.h5\", type_weights=None) # Load model\n",
        "\n",
        "        scaler = MaxAbsScaler()\n",
        "        scaler.fit(np.squeeze(dict_data[\"x_train\"][train_index], axis=2))\n",
        "        x_validation = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][validation_index], axis=2)), axis=2)\n",
        "\n",
        "\n",
        "        x_validation_type, x_validation_class = scattering_extract.predict(x_validation)\n",
        "\n",
        "        # Normalizing\n",
        "\n",
        "        transformer = MaxAbsScaler().fit(x_validation_type)\n",
        "        x_validation_type = transformer.transform(x_validation_type)\n",
        "\n",
        "        transformer = MaxAbsScaler().fit(x_validation_class)\n",
        "        x_validation_class = transformer.transform(x_validation_class)\n",
        "\n",
        "\n",
        "        final_prediction = []\n",
        "        final_groundTruth = []\n",
        "        for xi, xi_nd, yclass, ytype in zip(x_validation_type, x_validation_class, dict_data[\"y_train\"][\"classification\"][validation_index], dict_data[\"y_train\"][\"type\"][validation_index]):\n",
        "          \n",
        "            pred = bestModel.predict([np.expand_dims(xi, axis=0),np.expand_dims(xi_nd, axis=0)])\n",
        "            prediction = np.max(pred[1][0],axis=0) # Withou detection, the first index must be one (Related to classification)\n",
        "            groundTruth = np.max(yclass,axis=0)\n",
        "\n",
        "            final_prediction.append(prediction)\n",
        "            final_groundTruth.append(groundTruth) \n",
        "\n",
        "            del xi, yclass, ytype\n",
        "\n",
        "        event_type = np.min(np.argmax(dict_data[\"y_train\"][\"type\"][validation_index], axis=2), axis=1)\n",
        "\n",
        "        final_groundTruth = np.array(final_groundTruth)\n",
        "        final_prediction = np.array(final_prediction)\n",
        "    \n",
        "        # TODO: Handle scenarios with and without negative examples (without events). The current approach only makes sense for scenarios without negative examples\n",
        "        f1_macro.append([f1_score(final_groundTruth[event_type == 0] > threshold, final_prediction[event_type == 0] > threshold, average='macro', zero_division=0), \n",
        "                         f1_score(final_groundTruth[event_type == 1] > threshold, final_prediction[event_type == 1] > threshold, average='macro', zero_division=0)])\n",
        "        print(f\"Fold {fold}: F1 Macro avg: {np.average(f1_macro[-1]) * 100:.1f}\") \n",
        "\n",
        "    return np.argmax(np.average(f1_macro, axis=1)) + 1\n",
        "\n",
        "fold = choose_model(dict_data, folderPath)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:32<04:54, 32.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1: F1 Macro avg: 89.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:56<03:41, 27.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2: F1 Macro avg: 87.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [01:20<02:59, 25.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3: F1 Macro avg: 92.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:44<02:30, 25.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4: F1 Macro avg: 86.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [02:07<02:02, 24.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5: F1 Macro avg: 88.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [02:30<01:36, 24.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 6: F1 Macro avg: 88.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:55<01:12, 24.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 7: F1 Macro avg: 89.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [03:23<00:50, 25.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 8: F1 Macro avg: 91.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [03:47<00:25, 25.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 9: F1 Macro avg: 85.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [04:11<00:00, 25.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 10: F1 Macro avg: 86.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYMsQ-oQvb1"
      },
      "source": [
        "## Evaluates the identification\n",
        "\n",
        "This step generates a dict with the ground truth and the prediction for each test example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQDY22oqQvb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b179e01-c2b7-4163-c61c-b98d9e2232a1"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "foldFolderPath = folderPath + str(fold) + \"/\"\n",
        "\n",
        "train_index = np.load(foldFolderPath + \"train_index.npy\")\n",
        "validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n",
        "\n",
        "bestModel = ModelHandler.loadModel(foldFolderPath + \"model_without_detection.h5\", type_weights=None) # Load model\n",
        "\n",
        "scattering_extract = ModelHandler.loadModel(configs[\"FOLDER_PATH\"] + 'scattering_model.h5')\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaler.fit(np.squeeze(dict_data[\"x_train\"][train_index], axis=2))\n",
        "x_train = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][train_index], axis=2)), axis=2)\n",
        "x_validation = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_train\"][validation_index], axis=2)), axis=2)\n",
        "x_test = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_test\"], axis=2)), axis=2)\n",
        "\n",
        "\n",
        "x_test_type, x_test_class = scattering_extract.predict(x_test)\n",
        "\n",
        "# Normalizing\n",
        "\n",
        "transformer = MaxAbsScaler().fit(x_test_type)\n",
        "x_test_type = transformer.transform(x_test_type)\n",
        "        \n",
        "transformer = MaxAbsScaler().fit(x_test_class)\n",
        "x_test_class = transformer.transform(x_test_class)\n",
        "\n",
        "\n",
        "final_prediction = []\n",
        "final_groundTruth = []\n",
        "for xi, xi_nd, yclass, ytype in zip(x_test_type, x_test_class, dict_data[\"y_test\"][\"classification\"], dict_data[\"y_test\"][\"type\"]):\n",
        "    pred = bestModel.predict([np.expand_dims(xi, axis=0),np.expand_dims(xi_nd, axis=0)])\n",
        "    prediction = np.max(pred[1][0],axis=0)\n",
        "    groundTruth = np.max(yclass,axis=0)\n",
        "\n",
        "    final_prediction.append(prediction)\n",
        "    final_groundTruth.append(groundTruth) \n",
        "\n",
        "    del xi, yclass, ytype\n",
        "\n",
        "y = {}\n",
        "y[\"true\"] = final_groundTruth.copy()\n",
        "y[\"pred\"] = final_prediction.copy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usqx6Z-iQvb7"
      },
      "source": [
        "### F1 Score\n",
        "\n",
        "#### F1 Macro:\n",
        "$$\n",
        "\\begin{gather*}\n",
        "F1_{Macro} = \\frac{1}{Y} \\sum_{i=1}^{Y} \\frac{2 \\cdot tp_i}{2 \\cdot tp_i + fp_i + fn_i}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "#### F1 Micro:\n",
        "$$\n",
        "\\begin{gather*}\n",
        "F1_{Micro} = \\frac{2 \\cdot \\sum_{i=1}^{Y} tp_i}{\\sum_{i=1}^{Y} 2 \\cdot tp_i + fp_i + fn_i}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- $tp_i$: True positives classifications for appliance $i$\n",
        "- $fp_i$: False positives classifications for appliance $i$\n",
        "- $fn_i$: False negatives classifications for appliance $i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP4MWwFLQvb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12704281-b28b-4075-d239-423660ca54fd"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "threshold = 0.5\n",
        "f1_macro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='macro')\n",
        "f1_micro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='micro')\n",
        "\n",
        "print(f\"Fold {fold} - F1 Macro: {f1_macro * 100:.1f}, F1 Micro: {f1_micro * 100:.1f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - F1 Macro: 77.7, F1 Micro: 76.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJVFvT3LQvcA"
      },
      "source": [
        "### Accuracy (ACC)\n",
        "\n",
        "$$\n",
        "\\begin{gather*}\n",
        "ACC_i = \\frac{CCE_i}{TNE_i} \\\\ \\\\\n",
        "ACC = \\frac{1}{Y} \\sum_{i = 1}^{Y} ACC_i\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- $ACC_i$: Accuracy for appliance $i$\n",
        "- $CCE_i$: Load connected successfully identified\n",
        "- $TNE_i$: Total of connected events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQzNt8lTQvcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceaefc6d-709c-40c3-d25b-0e5c10ace7c9"
      },
      "source": [
        "threshold = 0.5\n",
        "\n",
        "correct_on = np.zeros((26,1))\n",
        "total_on = np.zeros((26,1))\n",
        "correct_off = np.zeros((26,1))\n",
        "total_off = np.zeros((26,1))\n",
        "correct_no_event = np.zeros((26,1))\n",
        "total_no_event = np.zeros((26,1))\n",
        "\n",
        "for ytype, ytrue, ypred in zip(dict_data[\"y_test\"][\"type\"], y[\"true\"], y[\"pred\"]):\n",
        "    event_type = np.min(np.argmax(ytype, axis=1))\n",
        "    if event_type == 0:\n",
        "        correct_on[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_on[ytrue > threshold] += 1\n",
        "    elif event_type == 1:\n",
        "        correct_off[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_off[ytrue > threshold] += 1\n",
        "    else:\n",
        "        correct_no_event[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n",
        "        total_no_event[ytrue > threshold] += 1\n",
        "\n",
        "acc_on = 100 * np.average(np.nan_to_num(correct_on/total_on))\n",
        "acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n",
        "acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n",
        "acc_total = 100 * np.average(np.nan_to_num((correct_on + correct_off + correct_no_event)/(total_on + total_off + total_no_event)))\n",
        "\n",
        "print(f\"Fold {fold} - Acc on: {acc_on:.1f}, Acc off: {acc_off:.1f}, Acc no event: {acc_no_event:.1f} Acc total: {acc_total:.1f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 - Acc on: 74.1, Acc off: 75.8, Acc no event: 0.0 Acc total: 76.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFz4ZTJQvcH"
      },
      "source": [
        "## Detection Metrics\n",
        "\n",
        "### D\n",
        "$$\n",
        "\\begin{gather*}\n",
        "D = \\frac{ \\sum_{i=1}^{A} |d(i) - ev(i)|}{A}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n",
        "- `d(i)`: Detection for appliance $i$\n",
        "- `ev(i)`: Ground truth detection for appliance $i$\n",
        "\n",
        "## PC\n",
        "\n",
        "$$\n",
        "\\begin{gather*}\n",
        "PC = \\frac{A}{N}\n",
        "\\end{gather*}\n",
        "$$\n",
        "\n",
        "- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n",
        "- `N`: Total of events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNHCZt9zQvcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa72508-db88-4a61-e2e7-56b3f8cdd2d4"
      },
      "source": [
        "from PostProcessing import PostProcessing\n",
        "from DataHandler import DataHandler\n",
        "\n",
        "postProcessing = PostProcessing(configs=configs)\n",
        "dataHandler = DataHandler(configs=configs)\n",
        "\n",
        "general_qtd_test = dict_data[\"y_test\"][\"group\"]\n",
        "\n",
        "foldFolderPath = folderPath + str(fold) + \"/\"\n",
        "\n",
        "train_index = np.load(foldFolderPath + \"train_index.npy\")\n",
        "\n",
        "bestModel = ModelHandler.loadModel(foldFolderPath + \"model_without_detection.h5\", type_weights=None) # Load model\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "scaler.fit(np.squeeze(dict_data[\"x_train\"][train_index], axis=2))\n",
        "x_test = np.expand_dims(scaler.transform(np.squeeze(dict_data[\"x_test\"], axis=2)), axis=2)\n",
        "x_test_type, x_test_class = scattering_extract.predict(x_test)\n",
        "\n",
        "\n",
        "# Normalizing\n",
        "\n",
        "transformer = MaxAbsScaler().fit(x_test_type)\n",
        "x_test_type = transformer.transform(x_test_type)\n",
        "        \n",
        "transformer = MaxAbsScaler().fit(x_test_class)\n",
        "x_test_class = transformer.transform(x_test_class)\n",
        "\n",
        "\n",
        "print(f\"-------------- FOLD {fold} ---------------\")\n",
        "pcMetric = postProcessing.checkModel2(bestModel, x_test_type, x_test_class, dict_data[\"y_test\"], general_qtd=general_qtd_test, print_error=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- FOLD 3 ---------------\n",
            "Total time: 30.28241867400152, Average Time: 0.03600763219262963\n",
            "LIT-SYN-1 PCmetric: (1.0, 1.0, 1.0)\n",
            "LIT-SYN-2 PCmetric: (0.9854014598540146, 0.8913043478260869, 0.9381818181818182)\n",
            "LIT-SYN-3 PCmetric: (0.9463087248322147, 0.9056603773584906, 0.9253246753246753)\n",
            "LIT-SYN-8 PCmetric: (0.9069767441860465, 0.5842696629213483, 0.7428571428571429)\n",
            "LIT-SYN-All PCmetric: (0.9568345323741008, 0.8419811320754716, 0.8989298454221165)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pcMetric\n",
        "\n",
        "pc_on = pcMetric[4][0]\n",
        "pc_off = pcMetric[4][1]\n",
        "pc_all = pcMetric[4][2]"
      ],
      "metadata": {
        "id": "RnWzpFITDgsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Results\n",
        "\n",
        "import tables\n",
        "import numpy as np\n",
        "\n",
        "row = [acc_on*0.01, acc_off*0.01, acc_total*0.01, f1_macro, f1_micro, pc_on, pc_off, pc_all]\n",
        "\n",
        "print(np.array(row))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AHvkiN0CBava",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eaf8f1b-2cf5-4f40-f549-ddc1a3b35c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.74072448 0.75806357 0.76396519 0.77668157 0.76444444 0.95683453\n",
            " 0.84198113 0.89892985]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THle5BPs_2ed",
        "outputId": "f4c0ba67-1df4-409c-ca77-ae0c7e4868fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HcyMNjZfDHEa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}