{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Measure_FLOPS_DeepDFML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.4.0\n",
        "!pip install keras==2.4.0\n",
        "!pip install kymatio\n",
        "!pip install tqdm\n",
        "!pip install iterative-stratification\n",
        "!pip install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N0MGCNSBsMHR",
        "outputId": "3971f4aa-5226-4cb1-f81e-e9d99f75bb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.8.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 67.5 MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 44.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 25.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68725 sha256=edee3453e16384160e52ea4472db16a6129317a4ad011f5f89cb7348af432c10\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.4.0\n",
            "  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.0) (3.13)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.15.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.32.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.37.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.8.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.2.0->keras==2.4.0) (3.2.0)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "Successfully installed keras-2.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kymatio\n",
            "  Downloading kymatio-0.2.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kymatio) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.1)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kymatio) (3.0.9)\n",
            "Installing collected packages: configparser, kymatio\n",
            "Successfully installed configparser-5.2.0 kymatio-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-90s9j6sJ-e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from absl import app, flags\n",
        "from easydict import EasyDict\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z42y6taXsP0W",
        "outputId": "b703b43c-4d6f-4d12-fa02-982915876965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append(\"drive/MyDrive/Scattering_Novo/src\")\n",
        "from DataHandler import DataHandler\n",
        "from ModelHandler import ModelHandler\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "\n",
        " \n",
        "configs = {\n",
        "    \"N_GRIDS\": 5, \n",
        "    \"SIGNAL_BASE_LENGTH\": 12800, \n",
        "    \"N_CLASS\": 26, \n",
        "    \"USE_NO_LOAD\": False, \n",
        "    \"USE_HAND_AUGMENTATION\": True,\n",
        "    \"MARGIN_RATIO\": 0.15, \n",
        "    \"DATASET_PATH\": \"drive/MyDrive/YOLO_NILM/Synthetic_Full_iHall.hdf5\",\n",
        "    \"TRAIN_SIZE\": 0.9,\n",
        "    \"FOLDER_PATH\": \"drive/MyDrive/Scattering_Novo/DeepDFML/Final/\", \n",
        "    \"FOLDER_DATA_PATH\": \"drive/MyDrive/Scattering_Novo/DeepDFML/Final/\", \n",
        "    \"N_EPOCHS_TRAINING\": 5000,\n",
        "    \"PERCENTUAL\": [1],\n",
        "    \"INITIAL_EPOCH\": 0,\n",
        "    \"TOTAL_MAX_EPOCHS\": 5000,\n",
        "    \"SNRdb\": None # Nível de ruído em db\n",
        "}\n",
        "\n",
        "def freeze(model, task_name='classification'):\n",
        "    for layer in model.layers:\n",
        "        if task_name in layer.name:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False\n",
        "\n",
        "    for layer in model.layers:\n",
        "        print(layer.name, layer.trainable)\n",
        "\n",
        "    return model\n",
        "\n",
        "def calculating_class_weights(y_true):\n",
        "    '''\n",
        "        Source: https://stackoverflow.com/questions/48485870/multi-label-classification-with-class-weights-in-keras\n",
        "    '''\n",
        "    from sklearn.utils.class_weight import compute_class_weight\n",
        "    number_dim = np.shape(y_true)[1]\n",
        "    weights = np.empty([number_dim, 2])\n",
        "    for i in range(number_dim):\n",
        "        weights[i] = compute_class_weight(class_weight='balanced', classes=[0.,1.], y=y_true[:, i])\n",
        "    return weights\n",
        "\n",
        "\n",
        "def reduce_dataset(X_all,ydet_all,ytype_all,yclass_all,percentual):\n",
        "    import numpy as np\n",
        "    max_index = int(percentual*X_all.shape[0])\n",
        "    np.random.seed(100)\n",
        "    index = np.random.randint(max_index,size=(max_index-1))\n",
        "    X_all = X_all[index]\n",
        "    ydet_all = ydet_all[index]\n",
        "    ytype_all = ytype_all[index]\n",
        "    yclass_all = yclass_all[index]\n",
        "\n",
        "    return X_all,ydet_all,ytype_all,yclass_all\n",
        "\n",
        "ngrids = configs[\"N_GRIDS\"]\n",
        "signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n",
        "trainSize = configs[\"TRAIN_SIZE\"]\n",
        "folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n",
        " \n",
        "dataHandler = DataHandler(configs)\n",
        "\n",
        "if not os.path.isfile(folderDataPath + \"sorted_aug_data_5_12800.p\"):\n",
        "    print(\"Sorted data not found, creating new file...\")\n",
        "    x, ydet, yclass, ytype, ygroup = dataHandler.loadData(hand_augmentation=configs[\"USE_HAND_AUGMENTATION\"], SNR=configs[\"SNRdb\"])\n",
        "    print(\"Data loaded\")\n",
        "\n",
        "    data_mskf = MultilabelStratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    strat_classes = np.max(yclass, axis=1)\n",
        "    train_index, test_index = next(data_mskf.split(x, strat_classes))\n",
        "\n",
        "    y_train = {\n",
        "        \"detection\": ydet[train_index], \n",
        "        \"type\": ytype[train_index], \n",
        "        \"classification\": yclass[train_index], \n",
        "        \"group\": ygroup[train_index]\n",
        "    }\n",
        "    \n",
        "    y_test = {\n",
        "        \"detection\": ydet[test_index], \n",
        "        \"type\": ytype[test_index], \n",
        "        \"classification\": yclass[test_index], \n",
        "        \"group\": ygroup[test_index]\n",
        "    }\n",
        "    \n",
        "    dict_data = {\n",
        "        \"x_train\": x[train_index], \n",
        "        \"x_test\": x[test_index], \n",
        "        \"y_train\": y_train, \n",
        "        \"y_test\": y_test\n",
        "    }\n",
        "\n",
        "    print(\"Data sorted\")\n",
        "\n",
        "    try:\n",
        "        os.mkdir(folderDataPath)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    pickle.dump(dict_data, open(folderDataPath + \"sorted_aug_data_5_12800.p\", \"wb\"))\n",
        "    print(\"Data stored\")\n",
        "else:\n",
        "    dict_data = pickle.load(open(folderDataPath + \"sorted_aug_data_5_12800.p\", \"rb\"))\n",
        "\n",
        "\n",
        "\n",
        "modelHandler = ModelHandler(configs)\n",
        " \n",
        "X_all = dict_data[\"x_train\"]\n",
        "ydet_all = dict_data[\"y_train\"][\"detection\"]\n",
        "ytype_all = dict_data[\"y_train\"][\"type\"]\n",
        "yclass_all = dict_data[\"y_train\"][\"classification\"]\n",
        "\n",
        "if configs[\"PERCENTUAL\"][0]!=1:\n",
        "        X_all,ydet_all,ytype_all,yclass_all = reduce_dataset(X_all,ydet_all,ytype_all,yclass_all,configs[\"PERCENTUAL\"][0])\n",
        "\n",
        "print(X_all.shape)\n",
        "print(dict_data[\"x_test\"].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyKE-nTLtYM2",
        "outputId": "fd57a874-4610-42fe-f413-216d140ef8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16649, 16640, 1)\n",
            "(4163, 16640, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Lambda, GlobalAveragePooling1D, Flatten, MaxPool1D, GlobalMaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from kymatio.keras import Scattering1D\n",
        "\n"
      ],
      "metadata": {
        "id": "xYJUZIMptnFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
        "\n",
        "\n",
        "def get_flops(model, batch_size=None):\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "\n",
        "    real_model = tf.function(model).get_concrete_function(tf.TensorSpec([batch_size] + model.inputs[0].shape[1:], model.inputs[0].dtype))\n",
        "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(real_model)\n",
        "\n",
        "    run_meta = tf.compat.v1.RunMetadata()\n",
        "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "    flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                            run_meta=run_meta, cmd='op', options=opts)\n",
        "    return flops.total_float_ops\n",
        "\n",
        "def get_flops_output(model, batch_size=None):\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    argument = (tf.TensorSpec([batch_size] + model.inputs[0].shape[1:], model.inputs[0].dtype, name=\"type_input\"),    \n",
        "            tf.TensorSpec([batch_size] + model.inputs[1].shape[1:], model.inputs[0].dtype, name=\"class_input\"))   \n",
        "    real_model = tf.function(model).get_concrete_function(argument)\n",
        "    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(real_model)\n",
        "\n",
        "    run_meta = tf.compat.v1.RunMetadata()\n",
        "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "    flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                            run_meta=run_meta, cmd='op', options=opts)\n",
        "    return flops.total_float_ops\n",
        "    "
      ],
      "metadata": {
        "id": "qPeWX7g9tw1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "fold = 1\n",
        "folderPath = configs[\"FOLDER_PATH\"] + str(fold) + \"/\"\n",
        "model = ModelHandler.loadModel(folderPath + 'best_model.h5')"
      ],
      "metadata": {
        "id": "qxQZ57yG1_lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets include the output network:"
      ],
      "metadata": {
        "id": "Xu3sAidowEfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flops_DFML = get_flops(model, 32)\n",
        "\n",
        "print(f\"DFML FLOPS: {flops_DFML / 10**9:.04} G\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAxcOBicxMT0",
        "outputId": "ff7f37fe-7527-432e-f959-06ff2338cab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:4893: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "DFML FLOPS: 7.643 G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN7pSJtErD6x",
        "outputId": "117f815f-2755-48a1-dab3-3b5b0947e6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 16640, 1)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 16632, 60)    600         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 16632, 60)    0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 4158, 60)     0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 4150, 40)     21640       max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 4150, 40)     0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1037, 40)     0           leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 1029, 40)     14440       max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 1029, 40)     0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 257, 40)      0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 249, 40)      14440       max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 249, 40)      0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 62, 40)       0           leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 54, 40)       14440       max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 54, 40)       0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, 13, 40)       0           leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 520)          0           max_pooling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 200)          104200      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 300)          156300      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 200)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 300)          0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 200)          0           leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 10)           5210        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 300)          0           leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 20)           4020        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 10)           0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 300)          90300       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 20)           0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 15)           165         leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 300)          0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 5)            105         leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 5, 3)         0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 130)          39130       leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "detection (Reshape)             (None, 5, 1)         0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "type (Softmax)                  (None, 5, 3)         0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classification (Reshape)        (None, 5, 26)        0           dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 464,990\n",
            "Trainable params: 464,990\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}