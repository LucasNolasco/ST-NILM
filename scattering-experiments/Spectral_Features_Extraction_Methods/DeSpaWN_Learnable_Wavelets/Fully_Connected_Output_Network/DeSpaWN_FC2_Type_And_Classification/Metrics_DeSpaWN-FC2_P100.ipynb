{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LcC32C0nzWJ0-vplzOF4upp0H7A6Y3G1","timestamp":1680543127414},{"file_id":"1mibVq8LCc_CrdFYrigDuApAbD-8OYO4Z","timestamp":1680310156402},{"file_id":"1pW31lJiMh35dKTLd-l7VojatBj77pQ3d","timestamp":1680103994549},{"file_id":"13uhX79D6PgKWzXQKp5fcT0tKZoywxZE8","timestamp":1680101774134},{"file_id":"1Nrq_5H_oXz-1d8Bto47-lxldJ8SIhpaA","timestamp":1680099209489},{"file_id":"1yNN-A1tW9R7xnKc_QdeHB3aHytB-kHFt","timestamp":1680029210436},{"file_id":"1sj06Iho_sjz6h2DUSDVEyWW0nLaYmtNx","timestamp":1680013327133},{"file_id":"1h8tyEde63QDSaf66cKZdrKk8QmqCz0xR","timestamp":1680005993229},{"file_id":"1-6q22cHiMGZGL7vh_Y6F4v3E6aphJ2dV","timestamp":1654529821350},{"file_id":"1GPqs1izycmkDVD5n6GgSy1DtCURypzT-","timestamp":1654528596832},{"file_id":"1cuqu_GSk66c2bg-i1YmHwkUeW1ujLhUT","timestamp":1654524075712},{"file_id":"1T5zGKNA-nI68K9Wru6qdpYuKmA9d7qY-","timestamp":1654518356066},{"file_id":"1McGSyXvSnyJ8YUkjg-VKG-_JClPFZLDi","timestamp":1654384652209},{"file_id":"1xzknJx-Ieeb7IL4VSc89fLJDhnesCdBS","timestamp":1654384467028},{"file_id":"1dY148WyRmZsJ0XJNHdUPrwCtYpDq6lk9","timestamp":1654286964737},{"file_id":"1qdcwiRqt2GqCt4IxdpCN6kRtMXvE-WGm","timestamp":1653344321268},{"file_id":"18iAYtreB5QrJDQxsqpXMLxtiA3idSr70","timestamp":1653342728047},{"file_id":"1yql6BpIjDRJJ_LTkNZP2SFjT7eqsT5R7","timestamp":1652320412957},{"file_id":"1RcUWqH9uz-xOBojM5MvbpaOd6izBxGsE","timestamp":1644124111506},{"file_id":"1Zr-Zo-H7PvannY2sqxQ4e8fEUHuCYBN-","timestamp":1635626859392},{"file_id":"https://github.com/LucasNolasco/DeepDFML-NILM/blob/master/notebooks/metrics_notebook.ipynb","timestamp":1625700111727}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"metadata":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"-GQaSQNQQvbo"},"source":["# Metrics\n","\n","Evaluates a trained model accordingly to the metrics specified on the paper"]},{"cell_type":"code","metadata":{"id":"YCTCtekCQyv0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359493577,"user_tz":180,"elapsed":23947,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"bc467b72-1112-4bb5-c8b2-9219d226083f"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install tensorflow==2.4.0\n","!pip install keras==2.4.0\n","!pip install kymatio\n","!pip install tqdm\n","!pip install iterative-stratification"],"metadata":{"id":"8RYHVHGDHAdo","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1682359810054,"user_tz":180,"elapsed":316482,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"eab56335-db11-4b12-9a24-36d98efddec0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.0\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras==2.4.0\n","  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.2/170.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.22.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (3.8.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.10.1)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (2.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (67.7.1)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.53.0)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.11.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (16.0.0)\n","Collecting tensorflow-estimator<2.11,>=2.10.0\n","  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.32.0)\n","Collecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.9.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.20.3)\n","  Downloading tensorflow-2.8.4-cp39-cp39-manylinux2010_x86_64.whl (498.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (23.3.3)\n","Collecting tensorboard<2.9,>=2.8\n","  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n","Collecting tensorflow-estimator<2.9,>=2.8\n","  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (4.5.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.4.0)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.8.3-cp39-cp39-manylinux2010_x86_64.whl (498.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.2-cp39-cp39-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.1-cp39-cp39-manylinux2010_x86_64.whl (498.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.7.4-cp39-cp39-manylinux2010_x86_64.whl (496.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.1/496.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.8,~=2.7.0rc0\n","  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.40.0)\n","Collecting flatbuffers<3.0,>=1.12\n","  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.12.2)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.7.3-cp39-cp39-manylinux2010_x86_64.whl (495.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.6/495.6 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.2-cp39-cp39-manylinux2010_x86_64.whl (495.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.6/495.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.1-cp39-cp39-manylinux2010_x86_64.whl (495.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.2/495.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.5-cp39-cp39-manylinux2010_x86_64.whl (464.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.3/464.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorboard<2.7,>=2.6.0\n","  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy>=1.9.1\n","  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h5py\n","  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.6.4-cp39-cp39-manylinux2010_x86_64.whl (464.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.3/464.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.3-cp39-cp39-manylinux2010_x86_64.whl (463.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.9/463.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorflow-2.6.2-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.6.1-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.7\n","  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.9/462.9 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting termcolor~=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting clang~=5.0\n","  Downloading clang-5.0.tar.gz (30 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting six~=1.15.0\n","  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n","Collecting tensorflow>=2.2.0\n","  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.12.0)\n","  Downloading tensorflow-2.5.3-cp39-cp39-manylinux2010_x86_64.whl (460.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.4/460.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio~=1.34.0\n","  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.17.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.0.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.27.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.2.3)\n","Collecting tensorboard~=2.5\n","  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting tensorboard~=2.5\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.4.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (6.4.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.2.2)\n","Building wheels for collected packages: termcolor, wrapt\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=62c205974046c00ae3bbd1ba3e3b26562b656a1b9a7e750946ac61c3896331c3\n","  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=75953 sha256=c079570307b331b4c45a0d61634c4ae78bb093bfb8b1d17de40adf7e29a7babd\n","  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n","Successfully built termcolor wrapt\n","Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, flatbuffers, tensorboard-data-server, six, numpy, keras-preprocessing, h5py, grpcio, absl-py, google-auth-oauthlib, tensorboard, tensorflow, keras\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.3.3\n","    Uninstalling flatbuffers-23.3.3:\n","      Successfully uninstalled flatbuffers-23.3.3\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.53.0\n","    Uninstalling grpcio-1.53.0:\n","      Successfully uninstalled grpcio-1.53.0\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","sqlalchemy 2.0.9 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","pydantic 1.10.7 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","pandas 1.5.3 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.19.5 which is incompatible.\n","optax 0.1.4 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","ml-dtypes 0.1.0 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n","matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","jax 0.4.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n","google-cloud-bigquery 3.9.0 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n","flax 0.6.8 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","astropy 5.2.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 grpcio-1.34.1 h5py-3.1.0 keras-2.4.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 numpy-1.19.5 six-1.15.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.5.3 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting kymatio\n","  Downloading kymatio-0.3.0-py3-none-any.whl (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.10.1)\n","Collecting configparser\n","  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.19.5)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.4.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from kymatio) (23.1)\n","Installing collected packages: configparser, kymatio\n","Successfully installed configparser-5.3.0 kymatio-0.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["configparser"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting iterative-stratification\n","  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.7\n"]}]},{"cell_type":"code","metadata":{"id":"ER7854z7Qvbw","executionInfo":{"status":"ok","timestamp":1682359831299,"user_tz":180,"elapsed":21257,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bc2b011-5301-4604-cab3-cfcf90d56e50"},"source":["import sys\n","sys.path.append(\"drive/MyDrive/Scattering_Novo/src\")\n","\n","# Scattering Parameters\n","J=10\n","Q=2\n","\n","\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_score, recall_score, multilabel_confusion_matrix \n","from sklearn.model_selection import train_test_split\n","from ModelHandler import ModelHandler\n","import pickle\n","import h5py\n","from sklearn.metrics import f1_score, precision_score, recall_score     \n","from tqdm import tqdm\n","import tensorflow as tf\n","\n","\n","J=10\n","Q=2\n","\n","configs = {\n","    \"N_GRIDS\": 5, \n","    \"SIGNAL_BASE_LENGTH\": 12800, \n","    \"N_CLASS\": 26, \n","    \"USE_NO_LOAD\": False, \n","    \"USE_HAND_AUGMENTATION\": False,\n","    \"MARGIN_RATIO\": 0.15, \n","    \"DATASET_PATH\": \"drive/MyDrive/Scattering_Novo/dataset_original/Synthetic_Full_iHall.hdf5\",\n","    \"TRAIN_SIZE\": 0.9,\n","    \"FOLDER_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FOLDER_DATA_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FEATURES_FILE_NAME\": \"features.mat\",\n","    \"OUTPUT_CLASSIFICATION_MODELS_PATH\": \"drive/MyDrive/DeSpaWN-main/classification/without_data_augmentation/\",\n","    \"N_EPOCHS_TRAINING\": 500,\n","    \"FEAT_NORMALIZATION\": False,\n","    \"TRAINING_FLAG\": 1,\n","    \"PERCENTUAL\": 1.0,\n","    \"INITIAL_EPOCH\": 0,\n","    \"TOTAL_MAX_EPOCHS\": 5000,\n","    \"SNRdb\": None # Nível de ruído em db\n","}\n","\n","GRIDSTYPE = \"GRIDS3\"\n","\n","folderPath = configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + \"FC_OUTPUT_TYPE_CLASS_\" + GRIDSTYPE + \"/\"\n","#folderPath = configs[\"TESTS_FOLDER\"] + 'Hybrid_P' + str(int(configs[\"PERCENTUAL\"]*100)) + '_J' + str(J) + '_Q' + str(Q) + '/'\n","folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n","signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n","ngrids = configs[\"N_GRIDS\"]\n","trainSize = configs[\"TRAIN_SIZE\"]\n","\n","dict_data = pickle.load(open(folderDataPath + \"data.p\", \"rb\")) # Load data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mT3znYfYUoiQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Features"],"metadata":{"id":"zOGsT7CAu01h"}},{"cell_type":"markdown","source":[],"metadata":{"id":"naYq1VpQ2TpR"}},{"cell_type":"code","source":["import scipy.io as sio\n"],"metadata":{"id":"6EcCDkm72UVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import os\n"],"metadata":{"id":"o7dX9z7REr__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Merging the Training Features"],"metadata":{"id":"ikbDlUByRfXh"}},{"cell_type":"code","source":["\n","if GRIDSTYPE==\"GRIDS3\":\n","  full_training_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_grids3.mat\"\n","  full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids3.mat\"\n","  training_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Training_Features_grids3\" \n","  path_base = \"features_train_grids3\"\n","elif GRIDSTYPE==\"GRIDS2\":\n","  full_training_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_grids2.mat\"\n","  full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids2.mat\"\n","  training_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Training_Features_grids2\" \n","  path_base = \"features_train_grids2\"\n","\n","  \n","if not os.path.exists(full_training_set_path):\n","    #feats= np.zeros([0,14])\n","    print(full_training_set_path)\n","    number_of_training_files= 75\n","    try: \n","      for cont in range(number_of_training_files+1): # Number of training files\n","        path = path_base + str(cont) + \".mat\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","              print(csv_path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                imported = sio.loadmat(csv_path)\n","                imported2 = imported['arr']\n","\n","                if cont==0:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","\n","    sio.savemat(full_training_set_path, {'arr': feats})\n","    print(\"Combined and saved!\")\n","\n","    print(feats.shape)\n","\n","    del feats, imported2\n"],"metadata":{"id":"ZLEizOieuyd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training data exists? \", os.path.exists(full_training_set_path))\n","print(\"Testing data exists? \", os.path.exists(full_testing_set_path))\n","print(full_testing_set_path)"],"metadata":{"id":"4ZiYkJdt4Aha","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359831301,"user_tz":180,"elapsed":18,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"273da04c-0445-4573-fab2-496610b7f292"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data exists?  True\n","Testing data exists?  True\n","drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/combined_features_testing_grids3.mat\n"]}]},{"cell_type":"code","source":["X_train = sio.loadmat(full_training_set_path)\n","X_train = X_train['arr']\n","y_train = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_train_.mat\")\n","y_train = y_train['arr']\n","\n","\n","\n"],"metadata":{"id":"Cet1vahnArG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"DRpzPOr7D-pU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832088,"user_tz":180,"elapsed":15,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"a4f3103a-6ed3-4f44-e0a5-96858f2ac418"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7575, 5, 10)\n","(7575, 5, 26)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"1KcaXe3jEq9F"}},{"cell_type":"markdown","source":["### Merging the testing features"],"metadata":{"id":"qDW8a_yHTi9E"}},{"cell_type":"code","source":["number_of_testing_files= 9\n","#features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Testing_Features\" \n","\n","\n","if GRIDSTYPE==\"GRIDS3\":\n","  full_training_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_grids3.mat\"\n","  full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids3.mat\"\n","  training_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Training_Features_grids3\" \n","  testing_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Testing_Features_grids3\" \n","  path_base = \"features_train_grids3\"\n","\n","elif GRIDSTYPE==\"GRIDS2\":\n","  full_training_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_grids2.mat\"\n","  full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids2.mat\"\n","  training_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Training_Features_grids2\" \n","  testing_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Testing_Features_grids2\" \n","  path_base = \"features_train_grids2\"\n","\n","if not os.path.exists(full_testing_set_path):\n","    try: \n","      for cont in range(number_of_testing_files): # Number of training files\n","        path = path_base + str(cont) + \".mat\"\n","\n","        if os.path.isfile(os.path.join(testing_features_folder_name,path)):\n","              csv_path = os.path.join(testing_features_folder_name,path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                imported = sio.loadmat(csv_path)\n","                imported2 = imported['arr']\n","                #imported2 = imported2.reshape([imported2.shape[1],])\n","\n","                if cont==0:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from testing dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","\n","    sio.savemat(configs[\"FOLDER_PATH\"] + \"combined_features_testing_grids2\" + \".mat\", {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","else:\n","    print(\"Testing data already merged.\")"],"metadata":{"id":"9596AQP1TfSb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832088,"user_tz":180,"elapsed":11,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"df740149-f277-4974-9c72-cdd4b2051182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing data already merged.\n"]}]},{"cell_type":"code","source":["os.path.exists(full_training_set_path)"],"metadata":{"id":"12vgJQPELbSP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832088,"user_tz":180,"elapsed":8,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"d99ebfa3-f378-4628-9ce3-beccec929498"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["print(full_testing_set_path)\n","\n","X_test  = sio.loadmat(full_testing_set_path)\n","X_test = X_test['arr']\n","y_test  = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_test_.mat\")\n","y_test = y_test['arr']\n"],"metadata":{"id":"cM8LfAgf06oW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832488,"user_tz":180,"elapsed":405,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"73a4b39a-c45d-4fc1-92fb-dba98bfe870d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/combined_features_testing_grids3.mat\n"]}]},{"cell_type":"code","source":["print(\"Shape of Classification Testing Features: \", X_test.shape)\n","print(\"Shape of Classification Testing Labels: \", y_test.shape)\n"],"metadata":{"id":"e121V7sb5_Nd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832489,"user_tz":180,"elapsed":11,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"a89283af-fd63-48f5-9933-26f8a4b2f398"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Classification Testing Features:  (841, 5, 10)\n","Shape of Classification Testing Labels:  (841, 5, 26)\n"]}]},{"cell_type":"markdown","source":["## Features Normalization"],"metadata":{"id":"xjqdD4-e1lgw"}},{"cell_type":"code","source":["if configs[\"FEAT_NORMALIZATION\"]:\n","  from sklearn.preprocessing import MultiLabelBinarizer\n","  from sklearn.preprocessing import MaxAbsScaler \n","  for grid in range(5): \n","    if grid==0:\n","      transformer = MaxAbsScaler().fit(X_test[:,grid,:])\n","      x_test_norm = transformer.transform(X_test[:,grid,:])\n","      transformer = MaxAbsScaler().fit(X_train[:,grid,:])\n","      X_train_norm = transformer.transform(X_train[:,grid,:])\n","    else:\n","      transformer = MaxAbsScaler().fit(X_test[:,grid,:])\n","      x_test_norm = np.append(x_test_norm, transformer.transform(X_test[:,grid,:]), axis=0 )\n","      transformer = MaxAbsScaler().fit(X_train[:,grid,:])\n","      X_train_norm = np.append( X_train_norm, transformer.transform(X_train[:,grid,:]), axis=0)\n","\n","  x_test_norm = x_test_norm.reshape([-1,5,10])\n","  X_train_norm = X_train_norm.reshape([-1,5,10])\n","\n","  print(x_test_norm.shape)\n","  print(X_train_norm.shape)\n","\n","  X_test = x_test_norm\n","  X_train = X_train_norm\n","\n","  del x_test_norm, X_train_norm"],"metadata":{"id":"hPjEY0uM1iH_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vomuxuyDZ9Pe"},"source":["## Choose best performing model\n","\n","At this point, the model with best performance under the validation set is chosen.\n","\n","In order to make this choice, the average between f1 macro is verified.\n","\n","$$\n","F_1 = \\frac{F1_{ON} + F1_{OFF} + F1_{NO EVENT}}{3}\n","$$"]},{"cell_type":"code","source":["# Loading Labels\n","yclass_all = y_train\n","ytype_all = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"ytype_train_.mat\")\n","ytype_all = ytype_all['arr']\n","\n","print(ytype_all.shape)\n","print(yclass_all.shape)"],"metadata":{"id":"IFkNBiwMVlKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359832489,"user_tz":180,"elapsed":9,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"ecb13434-c913-4f58-b271-82151caaac7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7575, 5, 3)\n","(7575, 5, 26)\n"]}]},{"cell_type":"code","source":["print(ytype_all.shape)\n","print(np.argmax(ytype_all, axis=2).shape)\n","print(np.min(np.argmax(ytype_all, axis=2), axis=1).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfRurfiS6MVU","executionInfo":{"status":"ok","timestamp":1682359832490,"user_tz":180,"elapsed":7,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"4cb61113-ed3d-40fe-82f3-c81068bfd78f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7575, 5, 3)\n","(7575, 5)\n","(7575,)\n"]}]},{"cell_type":"code","metadata":{"id":"uwe09boZZ81Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682359907294,"user_tz":180,"elapsed":74809,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"a3348cf2-8c22-404c-9d99-2ab81c09884b"},"source":["def choose_model(ytype_all, yclass_all, folderPath):\n","    from tqdm import tqdm\n","    from sklearn.preprocessing import MaxAbsScaler\n","    from sklearn.metrics import f1_score, precision_score, recall_score   \n","    from PostProcessing import PostProcessing\n","\n","    #scattering_extract = ModelHandler.loadModel(folderPath + 'scattering_model.h5') # Load scattering model\n","\n","    threshold = 0.5\n","    f1_macro, f1_micro = [], []\n","    for fold in tqdm(range(1, 3)):\n","        foldFolderPath = folderPath + str(fold) + \"/\"\n","        \n","        train_index = np.load(foldFolderPath + \"train_index.npy\")\n","        validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n","\n","        bestModel = ModelHandler.loadModel(foldFolderPath + \"model_without_detection.h5\", type_weights=None) # Load model\n","\n","\n","\n","        y_train, y_validation = {}, {}\n","        #y_train[\"detection\"] = ydet_all[train_index]\n","        #y_validation[\"detection\"] = ydet_all[validation_index]\n","        y_train[\"type\"] = ytype_all[train_index]\n","        y_validation[\"type\"] = ytype_all[validation_index]\n","        y_train[\"classification\"] = yclass_all[train_index]\n","        y_validation[\"classification\"] = yclass_all[validation_index]\n","\n","        # Selecting features for classification and type\n","\n","        x_train_class = X_train[train_index]\n","        x_validation_class = X_train[validation_index]\n","\n","        x_train_type = X_train[train_index]\n","        x_validation_type = X_train[validation_index]\n","\n","        # Flattening train\n","\n","        for k in range(x_train_class.shape[0]):\n","          if k==0:\n","            B = x_train_class[k].flatten()\n","            B = B.reshape([1,B.shape[0]])\n","          else:\n","            b = x_train_class[k].flatten()\n","            b = b.reshape([1,b.shape[0]])\n","            B = np.append(B, b, axis=0)\n","        x_train_class = B\n","        del B\n","\n","        for k in range(x_train_type.shape[0]):\n","          if k==0:\n","            B = x_train_type[k].flatten()\n","            B = B.reshape([1,B.shape[0]])\n","          else:\n","            b = x_train_type[k].flatten()\n","            b = b.reshape([1,b.shape[0]])\n","            B = np.append(B, b, axis=0)\n","        x_train_type = B\n","        del B\n","\n","        # Flattening validation\n","\n","        for k in range(x_validation_class.shape[0]):\n","          if k==0:\n","            B = x_validation_class[k].flatten()\n","            B = B.reshape([1,B.shape[0]])\n","          else:\n","            b = x_validation_class[k].flatten()\n","            b = b.reshape([1,b.shape[0]])\n","            B = np.append(B, b, axis=0)\n","        x_validation_class = B\n","        del B\n","\n","        for k in range(x_validation_type.shape[0]):\n","          if k==0:\n","            B = x_validation_type[k].flatten()\n","            B = B.reshape([1,B.shape[0]])\n","          else:\n","            b = x_validation_type[k].flatten()\n","            b = b.reshape([1,b.shape[0]])\n","            B = np.append(B, b, axis=0)\n","        x_validation_type = B\n","        del B\n","\n","        #print(x_validation_type.shape)\n","        #print(x_train_type.shape)\n","        #print(x_validation_class.shape)\n","        #print(x_train_class.shape)\n","        print(y_validation[\"classification\"].shape)\n","  \n","\n","        final_prediction = []\n","        final_groundTruth = []\n","        for xtype, xclass, yclass, ytype in zip(x_validation_type, x_validation_class, y_validation[\"classification\"], y_validation[\"type\"]):\n","\n","            pred = bestModel.predict([np.expand_dims(xtype, axis=0),np.expand_dims(xclass, axis=0)])\n","            prediction = np.max(pred[1][0],axis=0) \n","            groundTruth = np.max(yclass,axis=0)\n","\n","            final_prediction.append(prediction)\n","            final_groundTruth.append(groundTruth) \n","\n","            del xtype, xclass, yclass, ytype\n","\n","        event_type = np.min(np.argmax(y_validation[\"type\"], axis=2), axis=1)\n","\n","        final_groundTruth = np.array(final_groundTruth)\n","        final_prediction = np.array(final_prediction)\n","    \n","        f1_macro.append([f1_score(final_groundTruth[event_type == 0] > threshold, final_prediction[event_type == 0] > threshold, average='macro', zero_division=0), \n","                         f1_score(final_groundTruth[event_type == 1] > threshold, final_prediction[event_type == 1] > threshold, average='macro', zero_division=0)])\n","        print(f\"Fold {fold}: F1 Macro avg: {np.average(f1_macro[-1]) * 100:.1f}\") \n","\n","    return np.argmax(np.average(f1_macro, axis=1)) + 1\n","\n","fold = choose_model(ytype_all, yclass_all, folderPath)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["(733, 5, 26)\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 1/2 [00:38<00:38, 38.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Fold 1: F1 Macro avg: 11.9\n","(769, 5, 26)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [01:14<00:00, 37.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Fold 2: F1 Macro avg: 14.5\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"sqYMsQ-oQvb1"},"source":["## Evaluates the identification\n","\n","This step generates a dict with the ground truth and the prediction for each test example"]},{"cell_type":"code","metadata":{"id":"WQDY22oqQvb3"},"source":["from tqdm import tqdm\n","from sklearn.preprocessing import MaxAbsScaler\n","\n","foldFolderPath = folderPath + str(fold) + \"/\"\n","\n","train_index = np.load(foldFolderPath + \"train_index.npy\")\n","validation_index = np.load(foldFolderPath + \"validation_index.npy\")\n","\n","bestModel = ModelHandler.loadModel(foldFolderPath + \"model_without_detection.h5\", type_weights=None) # Load model\n","\n","\n","\n","y_train, y_validation = {}, {}\n","#y_train[\"detection\"] = ydet_all[train_index]\n","#y_validation[\"detection\"] = ydet_all[validation_index]\n","y_train[\"type\"] = ytype_all[train_index]\n","y_validation[\"type\"] = ytype_all[validation_index]\n","y_train[\"classification\"] = yclass_all[train_index]\n","y_validation[\"classification\"] = yclass_all[validation_index]\n","\n","    # Selecting features for classification and type\n","\n","x_train_class = X_train[train_index]\n","x_validation_class = X_train[validation_index]\n","\n","x_train_type = X_train[train_index]\n","x_validation_type = X_train[validation_index]\n","\n","x_test_class = X_test\n","x_test_type = X_test\n","\n","\n","    # Flattening train\n","\n","for k in range(x_train_class.shape[0]):\n","  if k==0:\n","    B = x_train_class[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_train_class[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_train_class = B\n","del B\n","\n","for k in range(x_train_type.shape[0]):\n","  if k==0:\n","    B = x_train_type[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_train_type[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_train_type = B\n","del B\n","\n","    # Flattening validation\n","\n","for k in range(x_validation_class.shape[0]):\n","  if k==0:\n","    B = x_validation_class[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_validation_class[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_validation_class = B\n","del B\n","\n","for k in range(x_validation_type.shape[0]):\n","  if k==0:\n","    B = x_validation_type[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_validation_type[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_validation_type = B\n","del B\n","\n","\n","    # Flattening Test\n","\n","for k in range(x_test_class.shape[0]):\n","  if k==0:\n","    B = x_test_class[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_test_class[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_test_class = B\n","del B\n","\n","for k in range(x_test_type.shape[0]):\n","  if k==0:\n","    B = x_test_type[k].flatten()\n","    B = B.reshape([1,B.shape[0]])\n","  else:\n","    b = x_test_type[k].flatten()\n","    b = b.reshape([1,b.shape[0]])\n","    B = np.append(B, b, axis=0)\n","x_test_type = B\n","del B\n","\n","\n","\n","final_prediction = []\n","final_groundTruth = []\n","for xi, xi_nd, yclass, ytype in zip(x_test_type, x_test_class, dict_data[\"y_test\"][\"classification\"], dict_data[\"y_test\"][\"type\"]):\n","    pred = bestModel.predict([np.expand_dims(xi, axis=0),np.expand_dims(xi_nd, axis=0)])\n","    prediction = np.max(pred[1][0],axis=0)\n","    groundTruth = np.max(yclass,axis=0)\n","\n","    final_prediction.append(prediction)\n","    final_groundTruth.append(groundTruth) \n","\n","    del xi, yclass, ytype\n","\n","y = {}\n","y[\"true\"] = final_groundTruth.copy()\n","y[\"pred\"] = final_prediction.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usqx6Z-iQvb7"},"source":["### F1 Score\n","\n","#### F1 Macro:\n","$$\n","\\begin{gather*}\n","F1_{Macro} = \\frac{1}{Y} \\sum_{i=1}^{Y} \\frac{2 \\cdot tp_i}{2 \\cdot tp_i + fp_i + fn_i}\n","\\end{gather*}\n","$$\n","\n","#### F1 Micro:\n","$$\n","\\begin{gather*}\n","F1_{Micro} = \\frac{2 \\cdot \\sum_{i=1}^{Y} tp_i}{\\sum_{i=1}^{Y} 2 \\cdot tp_i + fp_i + fn_i}\n","\\end{gather*}\n","$$\n","\n","- $tp_i$: True positives classifications for appliance $i$\n","- $fp_i$: False positives classifications for appliance $i$\n","- $fn_i$: False negatives classifications for appliance $i$"]},{"cell_type":"code","metadata":{"id":"AP4MWwFLQvb9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682360347951,"user_tz":180,"elapsed":302,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"1f28835f-0b61-41f9-b965-14c7b84b0181"},"source":["from sklearn.metrics import f1_score\n","\n","threshold = 0.5\n","f1_macro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='macro')\n","f1_micro = f1_score(np.array(y[\"true\"]) > threshold, np.array(y[\"pred\"]) > threshold, average='micro')\n","\n","print(f\"Fold {fold} - F1 Macro: {f1_macro * 100:.1f}, F1 Micro: {f1_micro * 100:.1f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 2 - F1 Macro: 13.8, F1 Micro: 19.5\n"]}]},{"cell_type":"markdown","metadata":{"id":"QJVFvT3LQvcA"},"source":["### Accuracy (ACC)\n","\n","$$\n","\\begin{gather*}\n","ACC_i = \\frac{CCE_i}{TNE_i} \\\\ \\\\\n","ACC = \\frac{1}{Y} \\sum_{i = 1}^{Y} ACC_i\n","\\end{gather*}\n","$$\n","\n","- $ACC_i$: Accuracy for appliance $i$\n","- $CCE_i$: Load connected successfully identified\n","- $TNE_i$: Total of connected events"]},{"cell_type":"code","metadata":{"id":"KQzNt8lTQvcD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682360359700,"user_tz":180,"elapsed":637,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"551c5a6b-5061-43b7-b599-e44b9dd46255"},"source":["threshold = 0.5\n","\n","correct_on = np.zeros((26,1))\n","total_on = np.zeros((26,1))\n","correct_off = np.zeros((26,1))\n","total_off = np.zeros((26,1))\n","correct_no_event = np.zeros((26,1))\n","total_no_event = np.zeros((26,1))\n","\n","for ytype, ytrue, ypred in zip(dict_data[\"y_test\"][\"type\"], y[\"true\"], y[\"pred\"]):\n","    event_type = np.min(np.argmax(ytype, axis=1))\n","    if event_type == 0:\n","        correct_on[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_on[ytrue > threshold] += 1\n","    elif event_type == 1:\n","        correct_off[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_off[ytrue > threshold] += 1\n","    else:\n","        correct_no_event[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_no_event[ytrue > threshold] += 1\n","\n","acc_on = 100 * np.average(np.nan_to_num(correct_on/total_on))\n","acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n","acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n","acc_total = 100 * np.average(np.nan_to_num((correct_on + correct_off + correct_no_event)/(total_on + total_off + total_no_event)))\n","\n","print(f\"Fold {fold} - Acc on: {acc_on:.1f}, Acc off: {acc_off:.1f}, Acc no event: {acc_no_event:.1f} Acc total: {acc_total:.1f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 2 - Acc on: 9.6, Acc off: 10.9, Acc no event: 0.0 Acc total: 10.7\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-6293edaa4ae2>:23: RuntimeWarning: invalid value encountered in true_divide\n","  acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n","<ipython-input-24-6293edaa4ae2>:24: RuntimeWarning: invalid value encountered in true_divide\n","  acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n"]}]},{"cell_type":"markdown","metadata":{"id":"LbFz4ZTJQvcH"},"source":["## Detection Metrics\n","\n","### D\n","$$\n","\\begin{gather*}\n","D = \\frac{ \\sum_{i=1}^{A} |d(i) - ev(i)|}{A}\n","\\end{gather*}\n","$$\n","\n","- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n","- `d(i)`: Detection for appliance $i$\n","- `ev(i)`: Ground truth detection for appliance $i$\n","\n","## PC\n","\n","$$\n","\\begin{gather*}\n","PC = \\frac{A}{N}\n","\\end{gather*}\n","$$\n","\n","- `A`: Total of events correctly detected ($\\pm$ 10 semi cycles tolerance)\n","- `N`: Total of events"]},{"cell_type":"code","metadata":{"id":"LNHCZt9zQvcJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682360585138,"user_tz":180,"elapsed":38800,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"e3bda832-97a8-4b74-b181-b3ac7329b3b7"},"source":["from PostProcessing import PostProcessing\n","from DataHandler import DataHandler\n","\n","postProcessing = PostProcessing(configs=configs)\n","dataHandler = DataHandler(configs=configs)\n","\n","general_qtd_test = dict_data[\"y_test\"][\"group\"]\n","\n","foldFolderPath = folderPath + str(fold) + \"/\"\n","\n","train_index = np.load(foldFolderPath + \"train_index.npy\")\n","\n","\n","\n","print(f\"-------------- FOLD {fold} ---------------\")\n","pcMetric = postProcessing.checkModel2(bestModel, x_test_type, x_test_class, dict_data[\"y_test\"], general_qtd=general_qtd_test, print_error=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------- FOLD 2 ---------------\n","Total time: 38.49111354699858, Average Time: 0.04576826818905895\n","LIT-SYN-1 PCmetric: (0.8222222222222222, 0.9210526315789473, 0.8674698795180723)\n","LIT-SYN-2 PCmetric: (0.7956204379562044, 0.855072463768116, 0.8254545454545454)\n","LIT-SYN-3 PCmetric: (0.6174496644295302, 0.710691823899371, 0.6655844155844156)\n","LIT-SYN-8 PCmetric: (0.47674418604651164, 0.5168539325842697, 0.49714285714285716)\n","LIT-SYN-All PCmetric: (0.6690647482014388, 0.7358490566037735, 0.7027348394768134)\n"]}]},{"cell_type":"code","source":["pcMetric\n","\n","pc_on = pcMetric[4][0]\n","pc_off = pcMetric[4][1]\n","pc_all = pcMetric[4][2]"],"metadata":{"id":"RnWzpFITDgsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the Results\n","\n","import tables\n","import numpy as np\n","\n","row = [acc_on*0.01, acc_off*0.01, acc_total*0.01, f1_macro, f1_micro, pc_on, pc_off, pc_all]\n","\n","print(np.array(row))\n","\n","\n"],"metadata":{"id":"AHvkiN0CBava","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682360647414,"user_tz":180,"elapsed":423,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"94e60b57-0499-49fc-911f-c9e5a76ad546"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.09596738 0.10944394 0.10660039 0.13820568 0.19493345 0.66906475\n"," 0.73584906 0.70273484]\n"]}]},{"cell_type":"code","source":["fold"],"metadata":{"id":"THle5BPs_2ed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682360656995,"user_tz":180,"elapsed":254,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"1b2c6094-cceb-4b6e-c010-446bc3d169bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"id":"HcyMNjZfDHEa"},"execution_count":null,"outputs":[]}]}