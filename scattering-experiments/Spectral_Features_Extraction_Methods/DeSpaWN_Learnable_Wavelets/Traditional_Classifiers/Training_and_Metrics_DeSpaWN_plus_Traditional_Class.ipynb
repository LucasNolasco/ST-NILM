{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uHHcfhl-pgybE7Cu1ZCB4X2mjtUkEaSB","timestamp":1680543266161}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4325,"status":"ok","timestamp":1680196642619,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"},"user_tz":180},"id":"S0dQGDeSzovV","outputId":"6115bd31-b34b-4683-a389-4c9559207a18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4QIVfpU0ybn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680196673140,"user_tz":180,"elapsed":30544,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"54f49671-7578-416a-c701-891e23b125fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.0\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras==2.4.0 in /usr/local/lib/python3.9/dist-packages (2.4.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.10.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (6.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.19.5)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (2.5.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (3.1.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.15.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.11.2)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.40.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.34.1)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0.dev2021032900)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.20.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.17.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.2.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (67.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (6.1.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.2.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kymatio in /usr/local/lib/python3.9/dist-packages (0.3.0)\n","Requirement already satisfied: configparser in /usr/local/lib/python3.9/dist-packages (from kymatio) (5.3.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.4.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from kymatio) (23.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from kymatio) (1.10.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.9/dist-packages (0.1.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.9/dist-packages (0.2.0)\n"]}],"source":["!pip install tensorflow==2.4.0\n","!pip install keras==2.4.0\n","!pip install kymatio\n","!pip install tqdm\n","!pip install iterative-stratification\n","!pip install scikit-multilearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FfVMPp26zwY"},"outputs":[],"source":["from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.model_selection import train_test_split, KFold\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import os\n","import pickle\n","import sys\n","sys.path.append(\"drive/MyDrive/Scattering_Novo/src\")\n","sys.path.append(\"drive/MyDrive/DeSpaWN-main\")\n","# Import Data\n","import scipy.io as sio\n","from DataHandler import DataHandler\n","from ModelHandler import ModelHandler\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from skmultilearn.model_selection import iterative_train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n","\n","J=10\n","Q=2\n","\n","configs = {\n","    \"N_GRIDS\": 5, \n","    \"SIGNAL_BASE_LENGTH\": 12800, \n","    \"N_CLASS\": 26, \n","    \"USE_NO_LOAD\": False, \n","    \"USE_HAND_AUGMENTATION\": False,\n","    \"MARGIN_RATIO\": 0.15, \n","    \"DATASET_PATH\": \"drive/MyDrive/Scattering_Novo/dataset_original/Synthetic_Full_iHall.hdf5\",\n","    \"TRAIN_SIZE\": 0.9,\n","    \"FOLDER_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FOLDER_DATA_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FEATURES_FILE_NAME\": \"features.mat\",\n","    \"OUTPUT_CLASSIFICATION_MODELS_PATH\": \"drive/MyDrive/DeSpaWN-main/classification/without_data_augmentation/\",\n","    \"N_EPOCHS_TRAINING\": 500,\n","    \"FEAT_NORMALIZATION\": False,\n","    \"TRAINING_FLAG\": 1,\n","    \"PERCENTUAL\": 1.0,\n","    \"INITIAL_EPOCH\": 0,\n","    \"TOTAL_MAX_EPOCHS\": 5000,\n","    \"SNRdb\": None \n","}\n","\n","def freeze(model, task_name='classification'):\n","    for layer in model.layers:\n","        if task_name in layer.name:\n","            layer.trainable = True\n","        else:\n","            layer.trainable = False\n","\n","    for layer in model.layers:\n","        print(layer.name, layer.trainable)\n","\n","    return model\n","\n","def calculating_class_weights(y_true):\n","    '''\n","        Source: https://stackoverflow.com/questions/48485870/multi-label-classification-with-class-weights-in-keras\n","    '''\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight(class_weight='balanced', classes=[0.,1.], y=y_true[:, i])\n","    return weights\n","\n","\n","def reduce_dataset(X_all,ydet_all,ytype_all,yclass_all,percentual):\n","    import numpy as np\n","    max_index = int(percentual*X_all.shape[0])\n","    np.random.seed(100)\n","    index = np.random.randint(max_index,size=(max_index-1))\n","    X_all = X_all[index]\n","    ydet_all = ydet_all[index]\n","    ytype_all = ytype_all[index]\n","    yclass_all = yclass_all[index]\n","\n","    return X_all,ydet_all,ytype_all,yclass_all\n","\n","ngrids = configs[\"N_GRIDS\"]\n","signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n","trainSize = configs[\"TRAIN_SIZE\"]\n","folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n"," \n","\n","folderPath = 'drive/MyDrive/Scattering_Novo/results_second_order/' + 'Hybrid2_P' + str(int(configs['PERCENTUAL']*100)) + '_J' + str(J) + '_Q' + str(Q) + '/'\n","#folderPath = configs[\"TESTS_FOLDER\"] + 'Hybrid_P' + str(int(configs[\"PERCENTUAL\"]*100)) + '_J' + str(J) + '_Q' + str(Q) + '/'\n","folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n","\n","trainSize = configs[\"TRAIN_SIZE\"]\n","\n","dict_data = pickle.load(open(folderDataPath + \"data.p\", \"rb\")) # Load data\n","\n","modelHandler = ModelHandler(configs)\n","\n","features_folder_name = configs[\"FOLDER_PATH\"] + \"Training_Features\" \n","\n"]},{"cell_type":"code","source":["training_flag=configs[\"TRAINING_FLAG\"]"],"metadata":{"id":"-9LZgez0FEEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Features"],"metadata":{"id":"zOGsT7CAu01h"}},{"cell_type":"markdown","source":[],"metadata":{"id":"naYq1VpQ2TpR"}},{"cell_type":"code","source":["import scipy.io as sio\n"],"metadata":{"id":"6EcCDkm72UVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o7dX9z7REr__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Merging the Training Features"],"metadata":{"id":"ikbDlUByRfXh"}},{"cell_type":"code","source":["full_training_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_grids2.mat\"\n","full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids2.mat\"\n","\n","training_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Training_Features_grids2\" \n","\n","if not os.path.exists(full_training_set_path):\n","    #feats= np.zeros([0,14])\n","    print(full_training_set_path)\n","    number_of_training_files= 75\n","    try: \n","      for cont in range(number_of_training_files+1): # Number of training files\n","        path = \"features_train_grids2\" + str(cont) + \".mat\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","              print(csv_path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                imported = sio.loadmat(csv_path)\n","                imported2 = imported['arr']\n","\n","                if cont==0:\n","                    feats = imported2\n","                else:\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","\n","    sio.savemat(full_training_set_path, {'arr': feats})\n","    print(\"Combined and saved!\")\n","\n","    print(feats.shape)\n","\n","    del feats, imported2\n"],"metadata":{"id":"ZLEizOieuyd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training data exists? \", os.path.exists(full_training_set_path))\n","print(\"Testing data exists? \", os.path.exists(full_testing_set_path))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZiYkJdt4Aha","executionInfo":{"status":"ok","timestamp":1680204761439,"user_tz":180,"elapsed":2,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"40ceebb2-a88f-420d-823d-f3eef0d6710d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data exists?  True\n","Testing data exists?  True\n"]}]},{"cell_type":"code","source":["X_train = sio.loadmat(full_training_set_path)\n","X_train = X_train['arr']\n","y_train = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_train_.mat\")\n","y_train = y_train['arr']\n"],"metadata":{"id":"Cet1vahnArG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRpzPOr7D-pU","executionInfo":{"status":"ok","timestamp":1680204766771,"user_tz":180,"elapsed":3,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"ad3c6ab7-af71-4540-9005-012402fe8b72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7575, 5, 10)\n","(7575, 5, 26)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"1KcaXe3jEq9F"}},{"cell_type":"markdown","source":["### Merging the testing features"],"metadata":{"id":"qDW8a_yHTi9E"}},{"cell_type":"code","source":["number_of_testing_files= 8\n","#features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Testing_Features\" \n","testing_features_folder_name = configs[\"FOLDER_DATA_PATH\"] + \"Testing_Features_grids2\" \n","\n","full_testing_set_path = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_grids2.mat\"\n","\n","if not os.path.exists(full_testing_set_path):\n","    try: \n","      for cont in range(number_of_testing_files+1): # Number of training files\n","        path = \"features_test_grids2\" + str(cont) + \".mat\"\n","\n","        if os.path.isfile(os.path.join(testing_features_folder_name,path)):\n","              csv_path = os.path.join(testing_features_folder_name,path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                imported = sio.loadmat(csv_path)\n","                imported2 = imported['arr']\n","                #imported2 = imported2.reshape([imported2.shape[1],])\n","\n","                if cont==0:\n","                    feats = imported2\n","                else:\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from testing dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","\n","    sio.savemat(configs[\"FOLDER_PATH\"] + \"combined_features_testing_grids2\" + \".mat\", {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","else:\n","    print(\"Testing data already merged.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9596AQP1TfSb","executionInfo":{"status":"ok","timestamp":1680204775271,"user_tz":180,"elapsed":291,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"50e811c0-c7c3-4d17-b532-9fcc2d13bbe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing data already merged.\n"]}]},{"cell_type":"code","source":["os.path.exists(full_training_set_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12vgJQPELbSP","executionInfo":{"status":"ok","timestamp":1680204793943,"user_tz":180,"elapsed":337,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"0c23fe2d-e01b-4eee-d07d-24b100af3c9d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["print(full_testing_set_path)\n","\n","X_test  = sio.loadmat(full_testing_set_path)\n","X_test = X_test['arr']\n","y_test  = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_test_.mat\")\n","y_test = y_test['arr']\n"],"metadata":{"id":"cM8LfAgf06oW","executionInfo":{"status":"ok","timestamp":1680204798107,"user_tz":180,"elapsed":4,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dde483f5-de97-429a-b994-17b14095af3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/combined_features_testing_grids2.mat\n"]}]},{"cell_type":"code","source":["print(\"Shape of Classification Testing Features: \", X_test.shape)\n","print(\"Shape of Classification Testing Labels: \", y_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e121V7sb5_Nd","executionInfo":{"status":"ok","timestamp":1680204804692,"user_tz":180,"elapsed":272,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"291dce5a-11d2-4a17-af79-08ae00c717f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Classification Testing Features:  (841, 5, 10)\n","Shape of Classification Testing Labels:  (841, 5, 26)\n"]}]},{"cell_type":"markdown","source":["## Features Normalization"],"metadata":{"id":"xjqdD4-e1lgw"}},{"cell_type":"code","source":["if configs[\"FEAT_NORMALIZATION\"]:\n","  from sklearn.preprocessing import MultiLabelBinarizer\n","  from sklearn.preprocessing import MaxAbsScaler \n","  for grid in range(5): \n","    if grid==0:\n","      transformer = MaxAbsScaler().fit(X_test[:,grid,:])\n","      x_test_norm = transformer.transform(X_test[:,grid,:])\n","      transformer = MaxAbsScaler().fit(X_train[:,grid,:])\n","      X_train_norm = transformer.transform(X_train[:,grid,:])\n","    else:\n","      transformer = MaxAbsScaler().fit(X_test[:,grid,:])\n","      x_test_norm = np.append(x_test_norm, transformer.transform(X_test[:,grid,:]), axis=0 )\n","      transformer = MaxAbsScaler().fit(X_train[:,grid,:])\n","      X_train_norm = np.append( X_train_norm, transformer.transform(X_train[:,grid,:]), axis=0)\n","\n","  x_test_norm = x_test_norm.reshape([-1,5,10])\n","  X_train_norm = X_train_norm.reshape([-1,5,10])\n","\n","  print(x_test_norm.shape)\n","  print(X_train_norm.shape)\n","\n","  X_test = x_test_norm\n","  X_train = X_train_norm\n","\n","  del x_test_norm, X_train_norm"],"metadata":{"id":"hPjEY0uM1iH_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"NKaDeZg81n-Y"}},{"cell_type":"code","source":["def train_all_traditional_classifiers(X_train,y_train_bin,X_test,y_test_bin,source_models_folder=configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"]):    \n","    # Classification Tests\n","    from sklearn.datasets import make_multilabel_classification\n","    from sklearn.multioutput import MultiOutputClassifier\n","    from sklearn.metrics import multilabel_confusion_matrix\n","    from sklearn.metrics import classification_report\n","\n","    # Logistic Regression\n","    from sklearn.linear_model import LogisticRegression\n","    LR_cls = MultiOutputClassifier(LogisticRegression()).fit(X_train, y_train_bin)\n","    y_pred_LR = LR_cls.predict(X_test)\n","    #LR_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_LR)\n","    #LR_report = classification_report(y_test_bin, y_pred_LR)\n","    #print(LR_report)\n","\n","    # Decision Tree\n","    from sklearn.tree import DecisionTreeClassifier\n","    TREE_cls = MultiOutputClassifier(DecisionTreeClassifier()).fit(X_train, y_train_bin)\n","    y_pred_TREE = TREE_cls.predict(X_test)\n","    #TREE_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_TREE)\n","    #TREE_report = classification_report(y_test_bin, y_pred_TREE)\n","    #print(TREE_report)\n","\n","    # Extra Tree (Extremely randomized trees)\n","    from sklearn.ensemble import BaggingClassifier\n","    from sklearn.tree import ExtraTreeClassifier\n","    extra_tree = ExtraTreeClassifier(random_state=0)\n","    EXTRA_cls = MultiOutputClassifier(BaggingClassifier(extra_tree, random_state=0)).fit(X_train, y_train_bin)\n","    y_pred_EXTRA = EXTRA_cls.predict(X_test)\n","    #EXTRA_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_EXTRA)\n","    #EXTRA_report = classification_report(y_test_bin, y_pred_EXTRA)\n","    #print(EXTRA_report)\n","\n","    # Ensemble Extra Tree\n","    from sklearn.ensemble import ExtraTreesClassifier\n","    ENSTREE_cls = MultiOutputClassifier(ExtraTreesClassifier(n_estimators=400, random_state=0)).fit(X_train, y_train_bin)\n","    y_pred_ENSTREE = ENSTREE_cls.predict(X_test)\n","    #ENSTREE_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_ENSTREE)\n","    #ENSTREE_report = classification_report(y_test_bin, y_pred_ENSTREE)\n","    #print(ENSTREE_report)\n","\n","    # K-Neighbors\n","    from sklearn.neighbors import KNeighborsClassifier\n","    KNN_cls = MultiOutputClassifier(KNeighborsClassifier(n_neighbors=2)).fit(X_train, y_train_bin)\n","    y_pred_KNN = KNN_cls.predict(X_test)\n","    #KNN_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_KNN)\n","    #KNN_report = classification_report(y_test_bin, y_pred_KNN)\n","    #print(KNN_report)\n","\n","    # MLP - Multi-Layer Perceptron\n","    from sklearn.neural_network import MLPClassifier\n","    MLP_cls = MultiOutputClassifier(MLPClassifier(random_state=1, max_iter=500)).fit(X_train, y_train_bin)\n","    y_pred_MLP = MLP_cls.predict(X_test)\n","\n","\n","    # Random Forest\n","    from sklearn.ensemble import RandomForestClassifier\n","    RF_cls = MultiOutputClassifier(RandomForestClassifier(max_depth=3, random_state=0)).fit(X_train, y_train_bin)\n","    y_pred_RF = RF_cls.predict(X_test)\n","    #RF_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_RF)\n","    #RF_report = classification_report(y_test_bin, y_pred_RF)\n","    #print(RF_report)\n","\n","    # Ridge Classifier\n","    from sklearn.linear_model import RidgeClassifier\n","    RID_cls = MultiOutputClassifier(RidgeClassifier()).fit(X_train, y_train_bin)\n","    y_pred_RID = RID_cls.predict(X_test)\n","    #RID_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_RID)\n","    #RID_report = classification_report(y_test_bin, y_pred_RID)\n","    #print(RID_report)\n","\n","    # Ridge Classifier with Cross-Validation\n","    from sklearn.linear_model import RidgeClassifierCV\n","    CVRID_cls = MultiOutputClassifier(RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1])).fit(X_train, y_train_bin)\n","    y_pred_CVRID = CVRID_cls.predict(X_test)\n","    #CVRID_confusion = multilabel_confusion_matrix(y_test_bin, y_pred_CVRID)\n","    #CVRID_report = classification_report(y_test_bin, y_pred_CVRID)\n","    #print(CVRID_report)\n","\n","    # Predictions \n","\n","    predictions = [y_pred_LR, y_pred_TREE, y_pred_EXTRA, y_pred_ENSTREE, y_pred_KNN, y_pred_MLP, y_pred_RF, y_pred_RID, y_pred_CVRID]\n","\n","    # Saving trained models\n","    # Execute these code lines manually\n","\n","    with open(source_models_folder + 'LR_cls.pkl', 'wb') as f:\n","        pickle.dump(LR_cls, f)\n","        \n","    with open(source_models_folder + 'TREE_cls.pkl', 'wb') as f:\n","        pickle.dump(TREE_cls, f)\n","        \n","    with open(source_models_folder + 'EXTRA_cls.pkl', 'wb') as f:\n","        pickle.dump(EXTRA_cls, f)\n","        \n","    with open(source_models_folder + 'ENSTREE_cls.pkl', 'wb') as f:\n","        pickle.dump(ENSTREE_cls, f)\n","\n","    with open(source_models_folder + 'KNN_cls.pkl', 'wb') as f:\n","        pickle.dump(KNN_cls, f)\n","\n","    with open(source_models_folder + 'MLP_cls.pkl', 'wb') as f:\n","        pickle.dump(MLP_cls, f)\n","\n","    with open(source_models_folder + 'RF_cls.pkl', 'wb') as f:\n","        pickle.dump(RF_cls, f)\n","\n","    with open(source_models_folder + 'RID_cls.pkl', 'wb') as f:\n","        pickle.dump(RID_cls, f)\n","\n","    with open(source_models_folder + 'CVRID_cls.pkl', 'wb') as f:\n","        pickle.dump(CVRID_cls, f)\n","    \n","    return predictions"],"metadata":{"id":"Kadro4P81sPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if training_flag==1:    \n","    from numpy.lib import shape_base\n","    for grid in range(configs[\"N_GRIDS\"]):\n","      \n","      output_address = configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + \"grid_\" + str(grid+1) + \"/\"\n","      try:\n","        os.mkdir(output_address)\n","      except:\n","        pass    \n","\n","      # Training all models for each grid\n","      print(\"Starting to train the classifiers of grid \", (grid+1) )\n","      predictions = train_all_traditional_classifiers(X_train=X_train[:,grid,:], y_train_bin=y_train[:,grid,:], X_test=X_test[:,grid,:], y_test_bin=y_test[:,grid,:], source_models_folder=output_address)\n","      print(\"Models of grid \" + str(grid+1) + \" trained sucessfully.\")\n","\n","\n","      if grid==0:\n","        y_pred_LR = predictions[0]\n","        y_pred_LR = y_pred_LR.reshape([1,y_pred_LR.shape[0],y_pred_LR.shape[1]])\n","\n","        y_pred_TREE = predictions[1]\n","        y_pred_TREE = y_pred_TREE.reshape([1,y_pred_TREE.shape[0],y_pred_TREE.shape[1]])\n","\n","        y_pred_EXTRA = predictions[2]\n","        y_pred_EXTRA = y_pred_EXTRA.reshape([1,y_pred_EXTRA.shape[0],y_pred_EXTRA.shape[1]])\n","\n","        y_pred_ENSTREE = predictions[3]\n","        y_pred_ENSTREE = y_pred_ENSTREE.reshape([1,y_pred_ENSTREE.shape[0],y_pred_ENSTREE.shape[1]])\n","\n","        y_pred_KNN = predictions[4]\n","        y_pred_KNN = y_pred_KNN.reshape([1,y_pred_KNN.shape[0],y_pred_KNN.shape[1]])\n","\n","        y_pred_MLP = predictions[5]\n","        y_pred_MLP = y_pred_MLP.reshape([1,y_pred_MLP.shape[0],y_pred_MLP.shape[1]])\n","\n","        y_pred_RF = predictions[6]\n","        y_pred_RF = y_pred_RF.reshape([1,y_pred_RF.shape[0],y_pred_RF.shape[1]])\n","\n","        y_pred_RID = predictions[7]\n","        y_pred_RID = y_pred_RID.reshape([1,y_pred_RID.shape[0],y_pred_RID.shape[1]])\n","\n","        y_pred_CVRID = predictions[8]\n","        y_pred_CVRID = y_pred_CVRID.reshape([1,y_pred_CVRID.shape[0],y_pred_CVRID.shape[1]])\n","\n","        shape_LR = y_pred_LR.shape\n","        shape_TREE = y_pred_TREE.shape\n","        shape_EXTRA = y_pred_EXTRA.shape\n","        shape_ENSTREE = y_pred_ENSTREE.shape\n","        shape_KNN = y_pred_KNN.shape\n","        shape_MLP = y_pred_MLP.shape\n","        shape_RF = y_pred_RF.shape\n","        shape_RID = y_pred_RID.shape\n","        shape_CVRID = y_pred_CVRID.shape\n","\n","      else:\n","\n","\n","        y_pred_LR = np.append(y_pred_LR, predictions[0].reshape(shape_LR), axis=0)\n","\n","        y_pred_TREE = np.append(y_pred_TREE, predictions[1].reshape(shape_TREE), axis=0)\n","\n","        y_pred_EXTRA = np.append(y_pred_EXTRA, predictions[2].reshape(shape_EXTRA), axis=0)\n","\n","        y_pred_ENSTREE = np.append(y_pred_ENSTREE, predictions[3].reshape(shape_ENSTREE), axis=0)\n","\n","        y_pred_KNN = np.append(y_pred_KNN, predictions[4].reshape(shape_KNN), axis=0)\n","\n","        y_pred_MLP = np.append(y_pred_MLP, predictions[5].reshape(shape_MLP), axis=0)\n","\n","        y_pred_RF = np.append(y_pred_RF, predictions[6].reshape(shape_RF), axis=0)\n","\n","        y_pred_RID = np.append(y_pred_RID, predictions[7].reshape(shape_RID), axis=0)\n","\n","        y_pred_CVRID = np.append(y_pred_CVRID, predictions[8].reshape(shape_CVRID), axis=0)\n","      \n","        # Predictions shape so far = (ngrids, nsxamples, nclass)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFBE9jcL7Haz","outputId":"93a8eab3-cd27-4ada-dcc6-749acd674e38","executionInfo":{"status":"ok","timestamp":1680206930067,"user_tz":180,"elapsed":2079466,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting to train the classifiers of grid  1\n","Models of grid 1 trained sucessfully.\n","Starting to train the classifiers of grid  2\n","Models of grid 2 trained sucessfully.\n","Starting to train the classifiers of grid  3\n","Models of grid 3 trained sucessfully.\n","Starting to train the classifiers of grid  4\n","Models of grid 4 trained sucessfully.\n","Starting to train the classifiers of grid  5\n","Models of grid 5 trained sucessfully.\n"]}]},{"cell_type":"markdown","source":["## Saving Predictions"],"metadata":{"id":"aXk7y56M1tks"}},{"cell_type":"code","source":["predictions_path = configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + \"Predictions_all_Classifiers/\"\n","\n","\n","if training_flag==1:    \n","    Predictions = [y_pred_LR, y_pred_TREE, y_pred_EXTRA, y_pred_ENSTREE, y_pred_KNN, y_pred_MLP, y_pred_RF, y_pred_RID, y_pred_CVRID]\n","\n","    try:\n","      os.mkdir(predictions_path)\n","    except:\n","      pass\n","\n","    sio.savemat(predictions_path + \"Predictions_all_Classifiers\" + \".mat\", {'Predictions': Predictions})"],"metadata":{"id":"920J_cDS1x6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Predictions = sio.loadmat(predictions_path + \"Predictions_all_Classifiers\" + \".mat\")\n","Predictions = Predictions[\"Predictions\"]"],"metadata":{"id":"z1X9qJouFOvE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"Sp0368x-1yWx"}},{"cell_type":"code","source":["def make_final_predictions(Y_pred,dict_data=dict_data):\n","    \n","    Y_pred = np.transpose(Y_pred, (1, 0, 2)) # Changing axis order, to (nexamples, ngrids, nclasses)\n","    \n","    final_prediction = []\n","    final_groundTruth = []\n","    for pred, yclass, ytype in zip(Y_pred, dict_data[\"y_test\"][\"classification\"], dict_data[\"y_test\"][\"type\"]):\n","        \n","        prediction = np.max(pred,axis=0)\n","        \n","        #pred = bestModel.predict([np.expand_dims(xi, axis=0),np.expand_dims(xi_nd, axis=0)])\n","        # Essa lista \"pred\" tem 2 elementos. O elemento na posição 0 (pred[0]) tem shape (1,5,3), que representa a detecção de tipo de evento para cada grid. Já pred[1] tem shape (1,5,26), e representa a classificação multi-label\n","        ##prediction = np.max(pred[1][0],axis=0)\n","        # pred[1][0].shape = (5,26), e prediction é a maior predição entre os grids\n","\n","        # yclass tem shape (5,26)\n","        groundTruth = np.max(yclass,axis=0) # groundTruth has shape (26,)\n","\n","        final_prediction.append(prediction)\n","        final_groundTruth.append(groundTruth) \n","\n","        del yclass, ytype\n","\n","        y = {}\n","    y[\"true\"] = final_groundTruth.copy()\n","    y[\"pred\"] = final_prediction.copy()\n","\n","    return y[\"true\"], y[\"pred\"]\n","\n"],"metadata":{"id":"7SmQoqpG10OW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usqx6Z-iQvb7"},"source":["### F1 Score\n","\n","#### F1 Macro:\n","$$\n","\\begin{gather*}\n","F1_{Macro} = \\frac{1}{Y} \\sum_{i=1}^{Y} \\frac{2 \\cdot tp_i}{2 \\cdot tp_i + fp_i + fn_i}\n","\\end{gather*}\n","$$\n","\n","#### F1 Micro:\n","$$\n","\\begin{gather*}\n","F1_{Micro} = \\frac{2 \\cdot \\sum_{i=1}^{Y} tp_i}{\\sum_{i=1}^{Y} 2 \\cdot tp_i + fp_i + fn_i}\n","\\end{gather*}\n","$$\n","\n","- $tp_i$: True positives classifications for appliance $i$\n","- $fp_i$: False positives classifications for appliance $i$\n","- $fn_i$: False negatives classifications for appliance $i$"]},{"cell_type":"code","source":["#from sklearn.metrics import f1_score\n","\n","from sklearn.metrics import f1_score, precision_score, recall_score \n","for k in range(len(Predictions)): # for each classifier\n","#for k in range(4): # for each classifier\n","    if k==0:\n","       y = {}\n","       y[\"true\"], y[\"pred\"] = make_final_predictions(Y_pred=Predictions[k])  \n","       y_true = np.array(y[\"true\"])\n","       y_pred = np.array(y[\"pred\"])\n","       threshold = 0.5\n","\n","       f1_macro = f1_score(y_true > threshold, y_pred > threshold, average='macro')\n","       f1_micro = f1_score(y_true > threshold, y_pred > threshold, average='micro')\n","\n","    else:\n","       y = {}\n","       y[\"true\"], y[\"pred\"] = make_final_predictions(Y_pred=Predictions[k])  \n","       y_true = np.array(y[\"true\"])\n","       y_pred = np.array(y[\"pred\"])\n","       threshold = 0.5\n","\n","       f1_macro = np.append(f1_macro, f1_score(y_true > threshold, y_pred > threshold, average='macro') )\n","       f1_micro = np.append(f1_micro, f1_score(y_true > threshold, y_pred > threshold, average='micro') )\n","\n","print(\"F1 macro: \", f1_macro)\n","print(\"F1 micro: \", f1_micro)\n","       \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DOcocGy89w3","executionInfo":{"status":"ok","timestamp":1680206932444,"user_tz":180,"elapsed":8,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"e7e2915d-1a98-469b-c0b8-2fdca8cfdd3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 macro:  [0.         0.29800857 0.26512197 0.28405482 0.1914795  0.04872303\n"," 0.         0.         0.        ]\n","F1 micro:  [0.         0.35613896 0.38427948 0.4069975  0.28276072 0.06544373\n"," 0.         0.         0.        ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"QJVFvT3LQvcA"},"source":["### Accuracy (ACC)\n","\n","$$\n","\\begin{gather*}\n","ACC_i = \\frac{CCE_i}{TNE_i} \\\\ \\\\\n","ACC = \\frac{1}{Y} \\sum_{i = 1}^{Y} ACC_i\n","\\end{gather*}\n","$$\n","\n","- $ACC_i$: Accuracy for appliance $i$\n","- $CCE_i$: Load connected successfully identified\n","- $TNE_i$: Total of connected events"]},{"cell_type":"code","metadata":{"id":"KQzNt8lTQvcD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680206932444,"user_tz":180,"elapsed":7,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"d4bee4f9-cbd0-40b9-bff3-85e7771e41a9"},"source":["threshold = 0.5\n","\n","correct_on = np.zeros((26,1))\n","total_on = np.zeros((26,1))\n","correct_off = np.zeros((26,1))\n","total_off = np.zeros((26,1))\n","correct_no_event = np.zeros((26,1))\n","total_no_event = np.zeros((26,1))\n","\n","for ytype, ytrue, ypred in zip(dict_data[\"y_test\"][\"type\"], y[\"true\"], y[\"pred\"]):\n","    event_type = np.min(np.argmax(ytype, axis=1))\n","    if event_type == 0:\n","        correct_on[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_on[ytrue > threshold] += 1\n","    elif event_type == 1:\n","        correct_off[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_off[ytrue > threshold] += 1\n","    else:\n","        correct_no_event[np.bitwise_and(ytrue > threshold, ypred > threshold)] += 1\n","        total_no_event[ytrue > threshold] += 1\n","\n","acc_on = 100 * np.average(np.nan_to_num(correct_on/total_on))\n","acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n","acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n","acc_total = 100 * np.average(np.nan_to_num((correct_on + correct_off + correct_no_event)/(total_on + total_off + total_no_event)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-106-eecc4f9c870a>:23: RuntimeWarning: invalid value encountered in true_divide\n","  acc_off = 100 * np.average(np.nan_to_num(correct_off/total_off))\n","<ipython-input-106-eecc4f9c870a>:24: RuntimeWarning: invalid value encountered in true_divide\n","  acc_no_event = 100 * np.average(np.nan_to_num(correct_no_event/total_no_event))\n"]}]}]}