{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1V1vBXqsAKTTN8BTrq62TNjCLRLOTCppO","timestamp":1681732723487},{"file_id":"1uHHcfhl-pgybE7Cu1ZCB4X2mjtUkEaSB","timestamp":1680543266161}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23555,"status":"ok","timestamp":1682343225895,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"},"user_tz":180},"id":"S0dQGDeSzovV","outputId":"1ee98e17-440f-4f57-fd64-6413eb5b86f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4QIVfpU0ybn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682343448512,"user_tz":180,"elapsed":123726,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"d8ee4643-c592-4af0-adea-75ed2591f262"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.5.0\n","  Downloading tensorflow-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.4/454.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.3.0)\n","Collecting termcolor~=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy~=1.19.2\n","  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting grpcio~=1.34.0\n","  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.6.3)\n","Collecting six~=1.15.0\n","  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n","Collecting flatbuffers~=1.12.0\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.40.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.2.0)\n","Collecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.20.3)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (2.12.2)\n","Collecting keras-preprocessing~=1.1.2\n","  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Collecting h5py~=3.1.0\n","  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.4.0)\n","Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n","  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n","  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard~=2.5\n","  Downloading tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.4.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.2.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (67.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.27.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.17.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (6.4.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n","Building wheels for collected packages: termcolor, wrapt\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=6256d71802fc61ed8b885f75163a9148db723ae640c31b537ae19b2fd03d5c4f\n","  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=75948 sha256=d0592a82e364b0d05f4a2d0dc303b6cf6949446f89b703db870bf610b3c9d668\n","  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n","Successfully built termcolor wrapt\n","Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, flatbuffers, tensorboard-data-server, six, numpy, keras-preprocessing, h5py, grpcio, absl-py, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: termcolor\n","    Found existing installation: termcolor 2.2.0\n","    Uninstalling termcolor-2.2.0:\n","      Successfully uninstalled termcolor-2.2.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.3.3\n","    Uninstalling flatbuffers-23.3.3:\n","      Successfully uninstalled flatbuffers-23.3.3\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: six\n","    Found existing installation: six 1.16.0\n","    Uninstalling six-1.16.0:\n","      Successfully uninstalled six-1.16.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.8.0\n","    Uninstalling h5py-3.8.0:\n","      Successfully uninstalled h5py-3.8.0\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.53.0\n","    Uninstalling grpcio-1.53.0:\n","      Successfully uninstalled grpcio-1.53.0\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.4.0\n","    Uninstalling absl-py-1.4.0:\n","      Successfully uninstalled absl-py-1.4.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","sqlalchemy 2.0.9 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","pydantic 1.10.7 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","polars 0.17.3 requires typing_extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","pandas 1.5.3 requires numpy>=1.20.3; python_version < \"3.10\", but you have numpy 1.19.5 which is incompatible.\n","optax 0.1.4 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","ml-dtypes 0.1.0 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n","matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.19.5 which is incompatible.\n","librosa 0.10.0.post2 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","jax 0.4.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n","google-cloud-bigquery 3.9.0 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n","flax 0.6.8 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n","bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n","astropy 5.2.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n","arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed absl-py-0.15.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 numpy-1.19.5 six-1.15.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras==2.4.0\n","  Using cached Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.19.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (6.0)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (2.5.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (1.10.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from keras==2.4.0) (3.1.0)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.15.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.15.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.20.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.11.2)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.1.2)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.34.1)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12.1)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.5.0.dev2021032900)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.12)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.40.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.7.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.27.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.17.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (67.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.4.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.2.3)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (6.4.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (1.26.15)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.2.0->keras==2.4.0) (3.2.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","Successfully installed keras-2.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.9/dist-packages (0.1.7)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from iterative-stratification) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.9/dist-packages (0.2.0)\n"]}],"source":["#!pip install tensorflow==2.4.0\n","!pip install tensorflow==2.5.0\n","!pip install keras==2.4.0\n","#!pip install kymatio\n","!pip install tqdm\n","!pip install iterative-stratification\n","!pip install scikit-multilearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FfVMPp26zwY"},"outputs":[],"source":["from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.model_selection import train_test_split, KFold\n","from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","import numpy as np\n","import os\n","import pickle\n","import sys\n","sys.path.append(\"drive/MyDrive/Scattering_Novo/src\")\n","sys.path.append(\"drive/MyDrive/DeSpaWN-main\")\n","# Import Data\n","import scipy.io as sio\n","#from DataHandler import DataHandler\n","#from ModelHandler import ModelHandler\n","#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","#from skmultilearn.model_selection import iterative_train_test_split\n","from sklearn.model_selection import KFold\n","from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n","\n","\n","configs = {\n","    \"N_GRIDS\": 5, \n","    \"SIGNAL_BASE_LENGTH\": 12800, \n","    \"N_CLASS\": 26, \n","    \"USE_NO_LOAD\": False, \n","    \"USE_HAND_AUGMENTATION\": False,\n","    \"MARGIN_RATIO\": 0.15, \n","    \"DATASET_PATH\": \"drive/MyDrive/Scattering_Novo/dataset_original/Synthetic_Full_iHall.hdf5\",\n","    \"TRAIN_SIZE\": 0.9,\n","    \"FOLDER_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FOLDER_DATA_PATH\": \"drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/\", \n","    \"FEATURES_FILE_NAME\": \"features.mat\",\n","    \"OUTPUT_CLASSIFICATION_MODELS_PATH\": \"drive/MyDrive/DeSpaWN-main/classification/without_data_augmentation/\",\n","    \"REQUIREMENTS_PATH\": \"drive/MyDrive/DeSpaWN-main/classification/\",\n","    \"N_EPOCHS_TRAINING\": 500,\n","    \"FEAT_NORMALIZATION\": True,\n","    \"TRAINING_FLAG\": 1,\n","    \"PERCENTUAL\": 1.0,\n","    \"INITIAL_EPOCH\": 0,\n","    \"TOTAL_MAX_EPOCHS\": 5000,\n","    \"SNRdb\": None \n","}\n","\n","def freeze(model, task_name='classification'):\n","    for layer in model.layers:\n","        if task_name in layer.name:\n","            layer.trainable = True\n","        else:\n","            layer.trainable = False\n","\n","    for layer in model.layers:\n","        print(layer.name, layer.trainable)\n","\n","    return model\n","\n","def calculating_class_weights(y_true):\n","    '''\n","        Source: https://stackoverflow.com/questions/48485870/multi-label-classification-with-class-weights-in-keras\n","    '''\n","    from sklearn.utils.class_weight import compute_class_weight\n","    number_dim = np.shape(y_true)[1]\n","    weights = np.empty([number_dim, 2])\n","    for i in range(number_dim):\n","        weights[i] = compute_class_weight(class_weight='balanced', classes=[0.,1.], y=y_true[:, i])\n","    return weights\n","\n","\n","def reduce_dataset(X_all,ydet_all,ytype_all,yclass_all,percentual):\n","    import numpy as np\n","    max_index = int(percentual*X_all.shape[0])\n","    np.random.seed(100)\n","    index = np.random.randint(max_index,size=(max_index-1))\n","    X_all = X_all[index]\n","    ydet_all = ydet_all[index]\n","    ytype_all = ytype_all[index]\n","    yclass_all = yclass_all[index]\n","\n","    return X_all,ydet_all,ytype_all,yclass_all\n","\n","ngrids = configs[\"N_GRIDS\"]\n","signalBaseLength = configs[\"SIGNAL_BASE_LENGTH\"]\n","trainSize = configs[\"TRAIN_SIZE\"]\n","folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n"," \n","\n","#folderPath = configs[\"TESTS_FOLDER\"] + 'Hybrid_P' + str(int(configs[\"PERCENTUAL\"]*100)) + '_J' + str(J) + '_Q' + str(Q) + '/'\n","folderDataPath = configs[\"FOLDER_DATA_PATH\"]\n","\n","trainSize = configs[\"TRAIN_SIZE\"]\n","\n","dict_data = pickle.load(open(folderDataPath + \"data.p\", \"rb\")) # Load data\n","\n","#modelHandler = ModelHandler(configs)\n","\n","features_folder_name = configs[\"FOLDER_PATH\"] + \"RAW_Features\" \n","\n"]},{"cell_type":"code","source":["training_flag=configs[\"TRAINING_FLAG\"]"],"metadata":{"id":"-9LZgez0FEEj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load Features"],"metadata":{"id":"zOGsT7CAu01h"}},{"cell_type":"markdown","source":[],"metadata":{"id":"naYq1VpQ2TpR"}},{"cell_type":"code","source":["import scipy.io as sio\n","import pickle\n"],"metadata":{"id":"6EcCDkm72UVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_training_set_path_G = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_G.mat\"\n","full_training_set_path_Gint = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_Gint.mat\"\n","full_training_set_path_H = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_training_H.mat\"\n","\n","full_testing_set_path_G = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_G.mat\"\n","full_testing_set_path_Gint = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_Gint.mat\"\n","full_testing_set_path_H = configs[\"FOLDER_DATA_PATH\"] + \"combined_features_testing_H.mat\"\n","\n","\n","training_features_folder_name = features_folder_name \n","testing_features_folder_name = features_folder_name "],"metadata":{"id":"o7dX9z7REr__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Merging the Training Features"],"metadata":{"id":"ikbDlUByRfXh"}},{"cell_type":"code","source":["'''\n","if os.path.exists(full_training_set_path_G):\n","  os.remove(full_training_set_path_G)\n","\n","if os.path.exists(full_training_set_path_H):\n","  os.remove(full_training_set_path_H)\n","'''\n","\n","if not os.path.exists(full_training_set_path_G):\n","    print(full_training_set_path_G)\n","    number_of_training_files= 76\n","    try: \n","      for cont in range(1,number_of_training_files+1): # Number of training files\n","\n","        path = \"G_train_RAW_\" + str(cont) + \".pkl\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","\n","              try:\n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                N = len(imported)\n","                \n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                  \n","                imported2 = np.zeros([N,10]) # 100 lines, one per example. 10 collumns, one per layer\n","\n","                for n in range(N): # for each sample\n","                  for k in range(10): # for each level\n","                    imported2[n,k] = np.mean( imported[n][k].reshape([imported[n][k].shape[1],]) )\n","                    \n"," \n","                if cont==1:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","    \n","    sio.savemat(full_training_set_path_G, {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","\n","if not os.path.exists(full_training_set_path_H):\n","    #feats= np.zeros([0,14])\n","    print(full_training_set_path_H)\n","    number_of_training_files= 76\n","    try: \n","      for cont in range(1,number_of_training_files+1): # Number of training files\n","      #for cont in range(1): # Number of training files\n","\n","        path = \"H_train_RAW_\" + str(cont) + \".pkl\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","              #print(csv_path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                N = len(imported)\n","                \n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                  \n","                imported2 = np.zeros([N,10]) # 100 lines, one per example. 10 collumns, one per layer\n","\n","                for n in range(N): # for each sample\n","                  for k in range(10): # for each level\n","                    imported2[n,k] = np.mean( imported[n][k].reshape([imported[n][k].shape[1],]) )\n","                    \n","                #imported = sio.loadmat(csv_path)\n","                #imported2 = imported['arr']\n","\n","                if cont==1:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","    \n","    sio.savemat(full_training_set_path_H, {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","\n","   # print(imported.shape)\n","\n","    #del feats, imported2\n"],"metadata":{"id":"ZLEizOieuyd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = sio.loadmat(full_training_set_path_H)\n","X_train = X_train['arr']\n","y_train = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_train_.mat\")\n","y_train = y_train['arr']\n"],"metadata":{"id":"Cet1vahnArG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRpzPOr7D-pU","executionInfo":{"status":"ok","timestamp":1682343606574,"user_tz":180,"elapsed":2,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"ca9ad2cd-2c0a-46f4-88d2-c9c1d1cbcf37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7575, 10)\n","(7575, 5, 26)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"1KcaXe3jEq9F"}},{"cell_type":"markdown","source":["### Merging the testing features"],"metadata":{"id":"qDW8a_yHTi9E"}},{"cell_type":"code","source":["'''\n","if os.path.exists(full_testing_set_path_G):\n","  os.remove(full_testing_set_path_G)\n","\n","if os.path.exists(full_testing_set_path_H):\n","  os.remove(full_testing_set_path_H)\n","'''\n","\n","if not os.path.exists(full_testing_set_path_G):\n","    #feats= np.zeros([0,14])\n","    print(full_testing_set_path_G)\n","    number_of_testing_files= 9\n","    try: \n","      for cont in range(1,number_of_testing_files+1): # Number of training files\n","      #for cont in range(1): # Number of training files\n","\n","        path = \"G_test_RAW_\" + str(cont) + \".pkl\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","              #print(csv_path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","                \n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                N = len(imported)\n","  \n","                imported2 = np.zeros([N,10]) # 100 lines, one per example. 10 collumns, one per layer\n","\n","                for n in range(N): # for each sample\n","                  for k in range(10): # for each level\n","                    imported2[n,k] = np.mean( imported[n][k].reshape([imported[n][k].shape[1],]) )\n","                    \n","                #imported = sio.loadmat(csv_path)\n","                #imported2 = imported['arr']\n","\n","                if cont==1:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","    \n","    sio.savemat(full_testing_set_path_G, {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","\n","if not os.path.exists(full_testing_set_path_H):\n","    #feats= np.zeros([0,14])\n","    print(full_testing_set_path_H)\n","    number_of_testing_files= 9\n","    try: \n","      for cont in range(1,number_of_testing_files+1): # Number of training files\n","      #for cont in range(1): # Number of training files\n","\n","        path = \"H_test_RAW_\" + str(cont) + \".pkl\"\n","\n","        if os.path.isfile(os.path.join(training_features_folder_name,path)):\n","              csv_path = os.path.join(training_features_folder_name,path)\n","\n","              #print(csv_path)\n","              #print(\"Extracting from \" + path)\n","              try:\n","\n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                N = len(imported)\n","                \n","                with open(csv_path, 'rb') as f:\n","                  imported = pickle.load(f) # List with 10 levels and for each of one of the 100 samples per pack\n","                  \n","                imported2 = np.zeros([N,10]) # 100 lines, one per example. 10 collumns, one per layer\n","\n","                for n in range(N): # for each sample\n","                  for k in range(10): # for each level\n","                    imported2[n,k] = np.mean( imported[n][k].reshape([imported[n][k].shape[1],]) )\n","                    \n","                #imported = sio.loadmat(csv_path)\n","                #imported2 = imported['arr']\n","\n","                if cont==1:\n","                    feats = imported2\n","                else:\n","                    #imported2 = imported2.reshape([int(imported2.shape[0]/14),14]) # The shape of a single file\n","                    feats = np.append(feats,  imported2, axis=0 )\n","              except:\n","                print(\"Fail merging file \" + path)\n","        else:\n","              print(\"File is not from training dataset\")\n","    except:\n","      print(\"Training Directory does not exist\")\n","    \n","    sio.savemat(full_testing_set_path_H, {'arr': feats})\n","    print(\"Combined and saved!\")\n","    del feats, imported2\n","\n","   # print(imported.shape)\n","\n","    #del feats, imported2\n"],"metadata":{"id":"9596AQP1TfSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_Z2jDDKN5JL","executionInfo":{"status":"ok","timestamp":1681752128316,"user_tz":180,"elapsed":4,"user":{"displayName":"EVERTON LUIZ DE AGUIAR","userId":"14644538063119763032"}},"outputId":"a89f0d04-58f4-443b-b2e0-0af69d38ed9f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["print(full_testing_set_path_H)\n","\n","X_test  = sio.loadmat(full_testing_set_path_H)\n","X_test = X_test['arr']\n","y_test  = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_test_.mat\")\n","y_test = y_test['arr']\n"],"metadata":{"id":"cM8LfAgf06oW","executionInfo":{"status":"ok","timestamp":1682343619751,"user_tz":180,"elapsed":1167,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4075aec-47b3-461e-8425-4d7583eb6e14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive/MyDrive/DeSpaWN-main/extracted_features/without_data_augmentation/Full_Dataset/combined_features_testing_H.mat\n"]}]},{"cell_type":"code","source":["print(\"Shape of Classification Testing Features: \", X_test.shape)\n","print(\"Shape of Classification Testing Labels: \", y_test.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e121V7sb5_Nd","executionInfo":{"status":"ok","timestamp":1682343623198,"user_tz":180,"elapsed":480,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"143f82e0-cfba-4bd5-b10a-3106136f6fb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of Classification Testing Features:  (841, 10)\n","Shape of Classification Testing Labels:  (841, 5, 26)\n"]}]},{"cell_type":"markdown","source":["## Features Normalization"],"metadata":{"id":"xjqdD4-e1lgw"}},{"cell_type":"code","source":["if configs[\"FEAT_NORMALIZATION\"]:\n","  from sklearn.preprocessing import MultiLabelBinarizer\n","  from sklearn.preprocessing import MaxAbsScaler \n","  transformer = MaxAbsScaler().fit(X_test)\n","  x_test_norm = transformer.transform(X_test)\n","  transformer = MaxAbsScaler().fit(X_train)\n","  X_train_norm = transformer.transform(X_train)\n","\n","  print(x_test_norm.shape)\n","  print(X_train_norm.shape)\n","\n","  X_test = x_test_norm\n","  X_train = X_train_norm\n","\n","  del x_test_norm, X_train_norm"],"metadata":{"id":"hPjEY0uM1iH_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682343626590,"user_tz":180,"elapsed":330,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"18216cdc-22e7-4b9f-db1f-c851eac67629"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(841, 10)\n","(7575, 10)\n"]}]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"NKaDeZg81n-Y"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","#from sklearn.grid_search import GridSearchCV\n","from sklearn.metrics import accuracy_score, log_loss\n","\n","#from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","#from sklearn.model_selection import train_test_split\n","\n"],"metadata":{"id":"D-ojZDIyWtY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed=1\n","models = [\n","            'ADB',\n","            'GBC',\n","            'RFC',\n","            'KNC',\n","            'SVC',\n","            'logisticRegression'\n","         ]\n","clfs = [\n","        AdaBoostClassifier(random_state=seed),\n","        GradientBoostingClassifier(random_state=seed),\n","        RandomForestClassifier(random_state=seed,n_jobs=-1),\n","        KNeighborsClassifier(n_jobs=-1),\n","        SVC(random_state=seed,probability=True),\n","        LogisticRegression(solver='newton-cg', multi_class='multinomial')\n","        ]\n","\n","params = {\n","            models[0]:{'learning_rate':[0.01], 'n_estimators':[150]},\n","            models[1]:{'learning_rate':[0.01],'n_estimators':[100], 'max_depth':[3],\n","                       'min_samples_split':[2],'min_samples_leaf': [2]},\n","            models[2]:{'n_estimators':[100], 'criterion':['gini'],'min_samples_split':[2],\n","                      'min_samples_leaf': [4]},\n","            models[3]:{'n_neighbors':[5], 'weights':['distance'],'leaf_size':[15]},\n","            models[4]: {'C':[100], 'tol': [0.005],\n","                       'kernel':['sigmoid']},\n","            models[5]: {'C':[2000], 'tol': [0.0001]}\n","         }"],"metadata":{"id":"XrPFmliCX-dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_train = (np.sum(y_train,axis=1)>0)\n","Y_train2 = np.zeros([Y_train.shape[0],Y_train.shape[1]])\n","\n","for k in range(Y_train.shape[0]): # for each line\n","  for kk in range(Y_train.shape[1]): # for each load\n","    if Y_train[k,kk]: # If true\n","      Y_train2[k,kk] = 1\n","\n","Y_train = Y_train2\n","del Y_train2\n","\n","print(Y_train[2019])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yur9HUotZjh2","executionInfo":{"status":"ok","timestamp":1682343712959,"user_tz":180,"elapsed":556,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"8262ec73-c124-4315-fc25-4d1dfa3b56a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0.]\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedShuffleSplit\n","\n","sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0)\n","\n","for train_index, test_index in sss.split(X_train, Y_train):\n","    n_x_train, n_x_val = X_train[train_index], X_train[test_index]\n","    n_y_train, n_y_val = Y_train[train_index], Y_train[test_index]\n","\n","n_x_test = X_test\n","y_test = 0\n","test_scores = []"],"metadata":{"id":"hcORsN6-YU5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_x_val.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1k7vWcHdUMd","executionInfo":{"status":"ok","timestamp":1682343722639,"user_tz":180,"elapsed":3,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"90937844-10b4-4452-d666-47aa1f6352f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(758, 10)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["print(n_y_train.shape)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7G0b-3sMeCga","executionInfo":{"status":"ok","timestamp":1682343873560,"user_tz":180,"elapsed":390,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"92a8a1c3-99ab-41b1-e377-461be805169c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(6817, 26)\n"]}]},{"cell_type":"code","source":["if not os.path.exists(configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + 'individual_traditional_cls.pkl'):\n","  classifiers = []\n","  Classifiers = {}\n","  col=0\n","  for col in range(26): # training a classifieer for each load\n","        for name, estimator in zip(models,clfs):\n","            print(name)\n","            clf = GridSearchCV(estimator, params[name], scoring='neg_mean_squared_error', refit='True', n_jobs=-1, cv=5)\n","            clf.fit(n_x_train, n_y_train[:,col])\n","\n","            #print(\"best params: \" + str(clf.best_params_))\n","            #print(\"best scores: \" + str(clf.best_score_))\n","            estimates = clf.predict_proba(n_x_test)\n","            y_test+=estimates\n","            acc = accuracy_score(n_y_val[:,col], clf.predict(n_x_val))\n","            #print(\"Accuracy: {:.4%}\".format(acc))\n","            classifiers.append(clf)\n","            test_scores.append((acc,clf.best_score_))\n","        Classifiers[col] = classifiers\n","        classifiers = []\n","  \n","  #import pickle\n","\n","  with open(configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + 'individual_traditional_cls.pkl', 'wb') as handle:\n","        pickle.dump(Classifiers, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","else:\n","  \n","  with open(configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + 'individual_traditional_cls.pkl', 'rb') as f:\n","        Classifiers = pickle.load(f)\n","\n"],"metadata":{"id":"6AOqQymXYfAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(Classifiers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5umd2ICgtfJ","executionInfo":{"status":"ok","timestamp":1682343941330,"user_tz":180,"elapsed":375,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"2f6718bd-e37d-4380-b041-b09841edafa9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["y_test  = sio.loadmat(configs[\"FOLDER_DATA_PATH\"] + \"yclass_test_.mat\")\n","y_test = y_test['arr']"],"metadata":{"id":"jcy5SsG2jfkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5PXsoXnkA_4","executionInfo":{"status":"ok","timestamp":1682344789174,"user_tz":180,"elapsed":3,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"799603b6-6159-44d4-90d1-cf200bf144ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(841, 5, 26)"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["# Predictions and Evaluation"],"metadata":{"id":"lKIPPioYqLq-"}},{"cell_type":"code","source":["# Test labels \n","Y_test = (np.sum(y_test,axis=1)>0)\n","Y_test2 = np.zeros([Y_test.shape[0],Y_test.shape[1]])\n","\n","for k in range(Y_test.shape[0]): # for each line\n","  for kk in range(Y_test.shape[1]): # for each load\n","    if Y_test[k,kk]: # If true\n","      Y_test2[k,kk] = 1\n","\n","Y_test = Y_test2\n","del Y_test2\n","\n","print(Y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ta2nlhgWi6BD","executionInfo":{"status":"ok","timestamp":1682344842057,"user_tz":180,"elapsed":2,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"eb10b37e-8b97-45b5-aad5-5542163aaed4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(841, 26)\n"]}]},{"cell_type":"markdown","source":["## Binary predictions for each classifier"],"metadata":{"id":"-JaiCMNw8Jk1"}},{"cell_type":"code","source":["Predictions = np.zeros((X_test.shape[0],26,len(Classifiers[0])))\n","Predictions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxFCtWtXmwn9","executionInfo":{"status":"ok","timestamp":1682351132161,"user_tz":180,"elapsed":346,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"70f1ef34-7182-4165-c66d-63f8247beddd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(841, 26, 6)"]},"metadata":{},"execution_count":143}]},{"cell_type":"code","source":["for load in range(26): # for each load\n","  for clf in range(len(Classifiers[load])):\n","    Predictions[:,load,clf] = Classifiers[load][clf].predict(X_test) \n"],"metadata":{"id":"7pm1J0EbkSlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","F1_score = []\n","\n","for cls in range(len(Classifiers[0])):\n","  y_pred = Predictions[:,:,cls]\n","\n","  F1_score.append(f1_score(Y_test, y_pred, average='weighted'))\n","\n","print(F1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx1gl9njnhwO","executionInfo":{"status":"ok","timestamp":1682351144660,"user_tz":180,"elapsed":320,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"76831108-64e4-4d4e-829c-99318c8fd4c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.0, 0.013314434845212384, 0.1304537651248588, 0.24411183490057223, 0.14635515041390457, 0.0018555658404470498]\n"]}]},{"cell_type":"markdown","source":["## Probabilistic accumulated predictions"],"metadata":{"id":"axRIUWF-8VmF"}},{"cell_type":"code","source":["predictions = 0\n","Predictions2 = []\n","for load in range(26): # for each load\n","  for clf in range(len(Classifiers[load])):\n","    predictions += Classifiers[load][clf].predict_proba(X_test) \n","  Predictions2.append(predictions)\n","  predictions=0\n","\n","Predictions2 = np.array(Predictions2)\n","proba_predictions = 1 / (1 + np.exp(-np.array(Predictions2)))\n"],"metadata":{"id":"1J-2ycDfuRUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["proba_predictions[:,:,1].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gt77f_Sw5LA2","executionInfo":{"status":"ok","timestamp":1682351722466,"user_tz":180,"elapsed":356,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"e5ae09ad-7e04-482d-ad03-11b443f89ee3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(26, 841)"]},"metadata":{},"execution_count":147}]},{"cell_type":"code","source":["#print(proba_predictions[16][800])\n","\n","#print(Y_test[800][16])\n","\n","threshold = 0.75\n","\n","binary_predictions = proba_predictions[:,:,1] >= threshold\n","binary_predictions = binary_predictions.astype(int)\n","\n","binary_predictions = binary_predictions.reshape([binary_predictions.shape[1], binary_predictions.shape[0]])\n","print(binary_predictions.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6tIPCGy3NGT","executionInfo":{"status":"ok","timestamp":1682351730612,"user_tz":180,"elapsed":325,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"2fabb95f-1e11-467e-f4c4-0ad6e9d30f26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(841, 26)\n"]}]},{"cell_type":"code","source":["f1 = f1_score(Y_test, binary_predictions, average='macro')\n","print(f1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNVuWVD16c5P","executionInfo":{"status":"ok","timestamp":1682351736659,"user_tz":180,"elapsed":392,"user":{"displayName":"Everton Luiz de Aguiar","userId":"16063652103765210785"}},"outputId":"3a2ae743-d27b-43a9-ace2-b321eb9b93f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0913649131001427\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EgwKTvNP7f4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1GmoBxOht6fl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving Predictions"],"metadata":{"id":"aXk7y56M1tks"}},{"cell_type":"code","source":["predictions_path = configs[\"OUTPUT_CLASSIFICATION_MODELS_PATH\"] + \"Predictions_LW_Traditional/\"\n","\n","try:\n","   os.mkdir(predictions_path)\n","except:\n","   pass\n","\n","\n","sio.savemat(predictions_path + \"Predictions_LW_Traditional\" + \".mat\", {'Predictions': Predictions})\n","sio.savemat(predictions_path + \"Predictions_LW_Traditional_probab\" + \".mat\", {'Predictions': Predictions2})"],"metadata":{"id":"920J_cDS1x6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Predictions = sio.loadmat(predictions_path + \"Predictions_LW_Traditional\" + \".mat\")\n","Predictions = Predictions[\"Predictions\"]\n","\n","\n","Predictions2 = sio.loadmat(predictions_path + \"Predictions_LW_Traditional_probab\" + \".mat\")\n","Predictions2 = Predictions2[\"Predictions\"]"],"metadata":{"id":"z1X9qJouFOvE"},"execution_count":null,"outputs":[]}]}